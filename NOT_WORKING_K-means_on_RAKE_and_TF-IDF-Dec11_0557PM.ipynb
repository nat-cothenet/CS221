{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CS221_11292018_0824 includes a RAKE class that applies RAKE on the input file\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "import gzip\n",
    "import statistics as stat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block reads meta data\n",
    "#reading meta data\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "meta_data = []\n",
    "# meta_path= '/Users/srishti/Desktop/meta_Books.json.gz'\n",
    "meta_path= '/Users/srishti/Desktop/meta_Health_and_Personal_Care.json.gz'\n",
    "counter= 1\n",
    "\n",
    "\n",
    "for line in parse(meta_path):\n",
    "    meta_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asin', 'description', 'title', 'imUrl', 'related', 'salesRank', 'categories']\n",
      "['asin', 'description', 'title', 'related', 'price', 'salesRank', 'imUrl', 'categories']\n",
      "['asin', 'title', 'imUrl', 'related', 'salesRank', 'categories']\n",
      "['asin', 'description', 'title', 'imUrl', 'related', 'salesRank', 'categories']\n",
      "['asin', 'description', 'title', 'brand', 'imUrl', 'related', 'categories']\n"
     ]
    }
   ],
   "source": [
    "# count=0\n",
    "# for i in meta_data:\n",
    "#     if 'related' in i.keys() and count < 5:\n",
    "#         print i.keys()\n",
    "# #         print asin_mother_dict[i]\n",
    "#         count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asin_mother_dict= collections.defaultdict(list)\n",
    "# asin_title_map = collections.defaultdict(list)\n",
    "\n",
    "# for i in meta_data:\n",
    "#     temp_dict={}\n",
    "#     temp_dict_all_parameters_known={}\n",
    "#     #Getting Titles\n",
    "#     if 'title' in i.keys():\n",
    "#         temp_dict['title']= i['title']\n",
    "#         temp_dict['all_param_known']='True'\n",
    "#         asin_mother_dict[i['asin']]= temp_dict\n",
    "#         asin_title_map[i['asin']] = i['title']\n",
    "        \n",
    "#     else:\n",
    "#         temp_dict['title']= None\n",
    "#         temp_dict['all_param_known']='False'\n",
    "#         asin_mother_dict[i['asin']]= temp_dict\n",
    "#         asin_title_map[i['asin']] = None\n",
    "        \n",
    "#     #Adding also_bought, also_viewed, bought_together and buy_after_viewing\n",
    "#     temp_dict={}\n",
    "#     temp_dict_all_parameters_known={}\n",
    "#     if 'related' in i.keys():\n",
    "#         if 'also_bought' in i['related']:\n",
    "#             temp_dict['also_bought']= i['related']['also_bought']\n",
    "#         else:\n",
    "#             temp_dict['also_bought']= None\n",
    "\n",
    "#         if 'also_viewed' in i['related']:\n",
    "#             temp_dict['also_viewed']= i['related']['also_viewed']\n",
    "#         else:\n",
    "#             temp_dict['also_viewed']= None\n",
    "        \n",
    "#         if 'bought_together' in i['related']:\n",
    "#             temp_dict['bought_together']= i['related']['bought_together']\n",
    "#         else:\n",
    "#             temp_dict['bought_together']= None\n",
    "        \n",
    "#         if 'buy_after_viewing' in i['related']:\n",
    "#             temp_dict['buy_after_viewing']= i['related']['buy_after_viewing']\n",
    "#         else:\n",
    "#             temp_dict['buy_after_viewing']= None\n",
    "        \n",
    "#         asin_mother_dict[i['asin']].update(temp_dict)\n",
    "#     else:\n",
    "#         temp_dict['all_param_known']='False'\n",
    "#         temp_dict['also_viewed']= None\n",
    "#         temp_dict['also_bought']= None\n",
    "#         temp_dict['bought_together']= None\n",
    "#         temp_dict['buy_after_viewing']= None\n",
    "        \n",
    "#         asin_mother_dict[i['asin']].update(temp_dict)\n",
    "\n",
    "#     #Adding categories\n",
    "#     #NOTE HEALTH & PERSONAL CARE IS A CATEGORY FOR ALL PRODUCT, NEED TO ADDRESS THAT\n",
    "#     temp_dict={}\n",
    "#     temp_dict_all_parameters_known={}\n",
    "#     if 'categories' in i.keys():\n",
    "#         temp_dict['categories']= i['categories'][0]\n",
    "#         asin_mother_dict[i['asin']].update(temp_dict)\n",
    "#     else:\n",
    "#         temp_dict['categories']= None\n",
    "#         temp_dict['all_param_known']='False'\n",
    "#         asin_mother_dict[i['asin']].update(temp_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count=0\n",
    "# for i in asin_mother_dict.keys():\n",
    "#     if count < 50:\n",
    "#         print i\n",
    "#         print asin_mother_dict[i]\n",
    "#         count+=1\n",
    "#     else:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "        \n",
    "#This block reads 5-core dataset\n",
    "# number_of_data_points_to_read= 10000 #Select number of data points to read\n",
    "counter= 0\n",
    "data=[]\n",
    "path= '/Users/srishti/Desktop/reviews_Health_and_Personal_Care_5.json'\n",
    "with open(path) as f:\n",
    "    for line in f:\n",
    "#         if counter <number_of_data_points_to_read:\n",
    "        data.append(json.loads(line))\n",
    "        counter+=1\n",
    "#         else: \n",
    "#             break\n",
    "\n",
    "X=[] #reivews\n",
    "Y=[] #ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<type 'list'>, {})\n"
     ]
    }
   ],
   "source": [
    "print asin_mother_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check if all the ASINs in meta data are there in data\n",
    "# set_asins= set(asin_mother_dict.keys())\n",
    "# set_not_in_master=set()\n",
    "\n",
    "# count =1\n",
    "# for i in data:\n",
    "#     if i['asin'] in set_asins:\n",
    "#         set_asins.remove(i['asin'])\n",
    "#     else:\n",
    "#         set_not_in_master.add(i['asin'])\n",
    "# # print len(set_not_in_master)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18534\n",
      "263034\n",
      "263034\n",
      "346355\n",
      "263032\n"
     ]
    }
   ],
   "source": [
    "print len(set_not_in_master)\n",
    "print len(asin_mother_dict.keys())\n",
    "print len(set(asin_mother_dict.keys()))\n",
    "print len(data)\n",
    "print len(meta_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_mother_dict= collections.defaultdict(list)\n",
    "asin_dict = collections.defaultdict(list)\n",
    "temp_dict_all_parameters_known={}\n",
    "counter=0\n",
    "for i in data:\n",
    "#     if counter < 10000:\n",
    "#         counter+=1\n",
    "    if i['asin'] not in asin_mother_dict.keys():\n",
    "        temp_dict={}\n",
    "        temp_dict['pos_reviews']=\"\"\n",
    "        temp_dict['neg_reviews']=\"\"\n",
    "        asin_mother_dict[i['asin']]=temp_dict\n",
    "\n",
    "#     asin_dict[i['asin']].append(i['reviewText'])\n",
    "    if i['overall'] >= 4:\n",
    "        temp_dict={}\n",
    "        temp_dict['pos_reviews']= asin_mother_dict[i['asin']]['pos_reviews']+ \" \"+ i['reviewText']\n",
    "        asin_mother_dict[i['asin']].update(temp_dict) \n",
    "    else:\n",
    "        temp_dict={}\n",
    "        temp_dict['neg_reviews']= asin_mother_dict[i['asin']]['neg_reviews']+ \" \"+ i['reviewText']\n",
    "        asin_mother_dict[i['asin']].update(temp_dict) \n",
    "#         asin_mother_dict[i['asin']]['neg_reviews'] = asin_mother_dict[i['asin']]['neg_reviews']+ \" \"+ i['reviewText']\n",
    "\n",
    "for i in asin_mother_dict.keys():\n",
    "    if asin_mother_dict[i]['pos_reviews']==\"\":\n",
    "        asin_mother_dict[i]['all_param_known']='False'\n",
    "    else:\n",
    "        asin_mother_dict[i]['all_param_known']='True'\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B000G6480G {'all_param_known': 'True', 'neg_reviews': u\" This seemed like it would work well, but does not.  It is difficult to use and takes several attempts at each hair.  A scissors is much faster, easier and more precise. The design and idea looks and sound great. In reality, though, its performace is only average to below average. I was really rooting for this clipper though. I'm going to be back to my trusty scissor. I tried the electric nose clippers but they were uncomfortable to use. I would not buy this clipper again nor recommend it.\", 'pos_reviews': u' Pros: Simple design, compact, durable, easy to keep clean, doesn\\'t get all gummed up with boogers (try one of the nose hair trimmers with the complicated trimming mechanisms and you\\'ll see what I mean)Cons: Sometimes \"pulls\" my nose hairs, could be a bit sharper This is great for getting at those singularly annoying wild hairs that grow with ever greater frequency out of the most unlikely places from my ever aging body.  Not a substitute for a motorized grooming device, but in certain situations, a lot safer, at least for me. from the photos it looks like it has sharp edges or something...it doesnt, and it is effective at what its meant to do. However, once in a while a hair will be to thick, or it\\'ll grab too many, so go slow or you can accidentally pull hairs out from the root.'}\n",
      "B0013J90AS {'all_param_known': 'True', 'neg_reviews': u' This really does work OK, but I found it numbed my throat as well which was kind of scary. it was probably me. In desperation I probably applied too much.I would suggest you try Clove Bud Essential Oil, available at Amazon. It works like a miracle and no nasty artificial ingredients.Really, though, this is a good product and a lot of people loved it from the reviews. I at first gave Orajel one star and then read another one star review -- where the person had the same response as me (more pain, swollen face)  -- and realized we had both suffered an allergic reaction. After some googling I realized that the allergic reaction to Orajel is actually quite common and can be very dangerous ... but the Orajel directions and warnings contain NO warning of this.  So I gave it one extra star because it clearly works great for some people, but only two stars because they need clearer warnings about dangerous allergic reactions, because at first when your tooth hurts worse you just think your toothache must be getting worse ... it was only because of the swelling, which I had never had before, that I realized Orajel itself must be the problem.** My initial review ***I had been suffering from a toothache a while ago, and ordered some Orajel. The toothache was gone by the time Orajel arrived in the mail but a few weeks later it started up again.  Relieved that I had some Orajel in stock, I rubbed some on the tooth and gums as directed ... and it hurt even more.  The pain increased significantly.  Also I noticed that my chin and cheek in the area of the tooth were significantly swollen.  I did not notice swelling a few weeks ago when I had the last toothache, and can only conclude it was caused by the Orajel.', 'pos_reviews': u' Orajel has always been a great product through out the years.  The taste isn\\'t to bad and it works instantly. It was my best friend when I had braces. I have sensitive teeth and just a little orajel reallyhelps.Orajel Maximum Strength Gel Oral Pain Reliever - .42 oz I am glad I was able to get \"Orajel Severe Toothache Pain Relief\" Especially dealing with a Wisdom Tooth, I was putting Salt Water on it, however, Oragel Cream is so much better, it really gets rid of the Pain right away and it heals irritated Gums soothing the entire area. \"Orajel Toothache Pain Relief Cream\" should not be use for more than 7 Days, according to the Directions on the Box, however, I was able to get to the Dentist before that. For those Emergencies, I am glad I found this Over-the-Counter Medication...Thank You  D.D.'}\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "for i in asin_mother_dict.keys():\n",
    "    if counter <2:\n",
    "        print i+ \" \"+ str(asin_mother_dict[i])\n",
    "        counter+=1\n",
    "        \n",
    "# 3812028492 {'neg_reviews': u'ADDED Real nice build to this razor. It\\'s hard to beat the German quality. However, I believe this razor is not aggressive enough. I actually got a  better shave with my $12.00 Lord. I just don\\'t get that close shave feeling, even after two passes. Maybe I should try a third pass, but I don\\'t think it\\'s going to matter. I\\'ve used both Feather and the Gillette 7 O\\'clock Sharp Edge blades. The new feather worked better this morning. I\\'ll try it for a few days more and see how it goes.ADDED I had this razor for 5 years.  Finally, the handle separated from the head of the razor while closing the razor head with the knob at the bottom.  There is a screw head that comes out of the handle, which the razor head attaches by surrounding the screw head with with plating.  I wasn\\'t abusing the razor or over-tightening it.  I was hoping to get more \"bang for the buck\" out of this razor.  I think the longer handle of this razor enables more force, while closing the razor.Since then, I\\'ve purchased the 34C (shorter handle).  I dropped the razor in the shower and, again, the head separated from the handle.  Is this a common design for plated DE razors?  Is it the chrome plating that\\'s susceptible to this?  I don\\'t know.I also found the spiral lines on the handle didn\\'t help get a firm grip on the razor.  My hands are occasionally soapy in the shower (surprise!) and every once in a while it would get away from me.  I don\\'t find this to be an issue with the 34C\\'s different design.The Merkur 38C is lackluster and I\\'ll be looking at other non-Merkur razors for my next purchase.', 'pos_reviews': u\"ADDED great quality, feels very good in my hand, very good weight, non slippery handle. it's a very good buy, cant wrong with this one. 34c is also very good, but shorter handle.ADDED Make no mistake this is a sturdy long handled razor. Feels substantial in your hand and the finish makes it easy to hold onto when wet. Easy to keep clean with the two piece design. Plating was beautiful and very good quality razor.ADDED My first safety razor, and I'll probably never have to buy another.My list of Amazon-acquired shaving supplies:Deluxe Stainless Steel Safety Razor and Shaving Brush Stand from Super Safety RazorsMerkur Classic Barbor Pole Long Handle Safety Razor #38 + 10 Free DE Razor BladesTruefitt & Hill Ultimate Comfort Pre-Shave Oil, 2 oz.Truefitt & Hill 1805 Aftershave BalmTweezerman  Men's Shaving BrushTruefitt & Hill 1805 Shave Cream JarOsma Styptic Pencil, Hemo StopFeather Hi-Stainless Platinum Double Edge Razor Blades 30 Ct100 Astra Superior Premium Platinum Double Edge Razor BladesADDED I made the jump to a safety razor because of the cost of disposable cartridges, but also because I thought i would enjoy to ritual of shaving this way.  All I can say is good choice by me!  It absolutely takes some getting used to (buy a Dabon by Pinaud styptic pencil at the same time) but once you get a handle on it you won't regret the switch.  This type of shaving produces a closer shave than i ever got with the most expensive multi-blade cartridges and now I'm down to one or two nicks a week, mostly because I forget to take my time.  Start out angling the razor by raising your wrist in order to avoid slicing yourself up and as you get used to it gradually lower the wrist and you'll see the true effectiveness of the razor.  This particular model has plenty of heft.  It looks and feels like the quality tool that it is.  Get yourself a badger hair bursh and some Taylor of Bond Street shaving cream, crank up the hot water and enjoy the gentlemanly ritual of a quality shave.  Pick up some after shave cream in order to soothe your skin because if you've been shaving with cartridges your face won't be used to the closeness of this shave.  Also, keep a few cartridges around for those mornings when you're running late.  Rush through a shave with this (or any other safety razor) and you'll pay the price.ADDED Being an extra large (and then some) person, I elected to get a razor with a bigger heavier handle, and this delivers.  Growing up appreciating German quality, I also found that to be appealing (it kinda delivers there).  All in all I'm happy, though not ecstatic about the purchase.Pros:Big and heavy as advertisedWell built at a functional levelLess aggressive head is forgiving for novicesIncludes 1 bladeCons:May be too big and heavy for someDetails are a bit rough for $50More advanced users may want to upgrade to a more aggressive head (if needed)Could include at least a 10 pack of blades for the priceI don't regret my purchase, though my next purchase likely would be a different model.  It's very forgiving for my novice skills, and that's a huge plus, but one that I'll outgrow (I hope!).  I really wish the details on the finish would wow me with craftsmanship, and it doesn't.  It's just average, and honestly less than I'd expect for $50.ADDED This is my first safety Razor.  it's worked very well so far.  It has a great weight to it and feels good in your hand.  It took me a long time to convince myself to spend $50+ on a razor, but it has been a good one so far.  The long handle is better for my because I have bear paws...ADDED Awesome razor. Went to this from a Mach 5 blade. SOOOO Much happier with this then any disposable blade I have ever used. The shave is incredibly close and smooth. The weight of the blade feels nice in you hand. Replacement blades are less then $0.50 each and last just as long as the disposables. Plus it's manly. Can't recommend enough.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_mother_dict_copy= asin_mother_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_mother_dict= asin_mother_dict_copy.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_title_map = collections.defaultdict(list)\n",
    "all_asins_set= set(asin_mother_dict.keys())\n",
    "\n",
    "for i in meta_data:\n",
    "    temp_dict={}\n",
    "    temp_dict_all_parameters_known={}\n",
    "    if i['asin'] in all_asins_set:\n",
    "        all_asins_set.remove(i['asin'])\n",
    "        #Getting Titles\n",
    "        if 'title' in i.keys():\n",
    "            temp_dict['title']= i['title']\n",
    "#             print i['asin']\n",
    "#             print asin_mother_dict[i['asin']]\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "    #         asin_title_map[i['asin']] = i['title']\n",
    "\n",
    "        else:\n",
    "            temp_dict['title']= None\n",
    "            temp_dict['all_param_known']='False'\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "            asin_title_map[i['asin']] = None\n",
    "\n",
    "        #Adding also_bought, also_viewed, bought_together and buy_after_viewing\n",
    "        temp_dict={}\n",
    "        temp_dict_all_parameters_known={}\n",
    "        if 'related' in i.keys():\n",
    "            if 'also_bought' in i['related']:\n",
    "                temp_dict['also_bought']= i['related']['also_bought']\n",
    "            else:\n",
    "                temp_dict['also_bought']= None\n",
    "\n",
    "            if 'also_viewed' in i['related']:\n",
    "                temp_dict['also_viewed']= i['related']['also_viewed']\n",
    "            else:\n",
    "                temp_dict['also_viewed']= None\n",
    "\n",
    "            if 'bought_together' in i['related']:\n",
    "                temp_dict['bought_together']= i['related']['bought_together']\n",
    "            else:\n",
    "                temp_dict['bought_together']= None\n",
    "\n",
    "            if 'buy_after_viewing' in i['related']:\n",
    "                temp_dict['buy_after_viewing']= i['related']['buy_after_viewing']\n",
    "            else:\n",
    "                temp_dict['buy_after_viewing']= None\n",
    "\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "        else:\n",
    "            temp_dict['all_param_known']='False'\n",
    "            temp_dict['also_viewed']= None\n",
    "            temp_dict['also_bought']= None\n",
    "            temp_dict['bought_together']= None\n",
    "            temp_dict['buy_after_viewing']= None\n",
    "\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "\n",
    "        #Adding categories\n",
    "        #NOTE HEALTH & PERSONAL CARE IS A CATEGORY FOR ALL PRODUCT, NEED TO ADDRESS THAT\n",
    "        temp_dict={}\n",
    "        temp_dict_all_parameters_known={}\n",
    "        if 'categories' in i.keys():\n",
    "            temp_dict['categories']= i['categories'][0]\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "        else:\n",
    "            temp_dict['categories']= None\n",
    "            temp_dict['all_param_known']='False'\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B006OC3JY2\n",
      " Someone told my boyfriend that nopalea helped them with multiple chemical sensitivity. This is cheaper and has no added sugars. Maybe it helps to cut down on neurotoxicity?It tastes alright, not great but not bad. I also recommend the nopal capsules on amazon. this product has a great taste and I noticed a difference within just a few doses, sort of pricey but I think it will be worth it!\n",
      "B0014UFVBI\n",
      " I get an upset stomach multiple times a week, my cousin told me to try this so I did. I'm glad I did. I take two elm's and my stomach is feeling a whole lot better within a half hour. This is something everyone should have in their home. I have all my clients coming to me use it  for digestive aid or heartburn. It helps better than most commercial digestive aids. This is a great &#34;natural&#34; holistic remedy. I have a cat with chronic kidney disease and his tummy gets pretty &#34;sour&#34; these days. He gets the maximum dose of Pepcid recommended by my holistic vet, but the acid reflux still persists some days, so I am now giving him this with his food. Directions for preparation are on the web site &#34;Feline CRF&#34;. If he is not too enthusiastic about eating, I make this into a liquid and give it to him with a syringe. It apparently has a light sweet taste. It really coats his esophagus and stomach, because he starts eating within an hour or so. As everyone knows, its great for people too! The vet put my dog on this when he was  experiencing  IBS.  Now I keep it on hand for soothing his stomach if he has a  bout with throwing up. I let nature handle it at first but if he continues with throwing up this really helps him out. If he has diarrhea I give him priobiotics. The vet thought he would have a chronic problem but he hasn't had it for a few years now. wonderful supplement for my arsenal against leaky gut I have been having mild gastritis for over a year now and I have gotten pretty tired of it. Bought this in hope that it will allow my insides to heal. I have tried a lot of natural remedies for my gastritis but I feel this product has actually worked. I drink two cups a day mixed with Glutamine powder. Seems to be working! Just recovering from a reaction to taking antibiotics.  Took along with Healthy GUT and got fast results.  This is good to help build up your immune system I used this to get off proton pump inhibitors.  Used it in conjunction with ranitidine.  Finally after over a year of trying I am completely off the proton pump inhibitor and have weaned almost off ranitidine (one every few weeks) and will soon only take the slippery elm when needed.Anyone who's tried to get off proton pump inhibitors who have been on them for years knows how difficult it is.\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in asin_mother_dict.keys():\n",
    "    if count < 2:\n",
    "        print i\n",
    "        print asin_mother_dict[i]['pos_reviews']\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_mother_dict_copy= asin_mother_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18276\n"
     ]
    }
   ],
   "source": [
    "for i in asin_mother_dict.keys():\n",
    "    if asin_mother_dict[i]['all_param_known']!= 'True':\n",
    "        del(asin_mother_dict[i])\n",
    "print len(asin_mother_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "embeddings_index = dict()\n",
    "f = open('/Users/srishti/Google Drive/000_7th Quarter/CS221/Project/untouched_data/glove.6B/glove.6B.50d.txt')\n",
    "for line in f:# first element of each line is the word and remaining elements are numerical represenation of each line \n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs# creates a dict of words and numerical representation of that word\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02648  ,  0.33737  ,  0.065667 , -0.11609  ,  0.41651  ,\n",
       "       -0.21142  , -0.69582  ,  0.2822   , -0.36077  , -0.13822  ,\n",
       "        0.012094 ,  0.086227 , -0.84638  ,  0.057195 ,  1.1582   ,\n",
       "        0.14703  , -0.0049197, -0.24899  , -0.96014  , -0.3038   ,\n",
       "        0.23972  ,  0.21058  ,  0.40608  ,  0.17789  ,  0.55253  ,\n",
       "       -1.6357   , -0.17784  , -0.45222  ,  0.45805  ,  0.14239  ,\n",
       "        3.7087   ,  0.40289  , -0.4083   , -0.29304  ,  0.030857 ,\n",
       "       -0.15361  ,  0.10607  ,  0.63397  ,  0.12397  , -0.25349  ,\n",
       "       -0.10344  ,  0.0069768, -0.17328  ,  0.35536  , -0.46369  ,\n",
       "        0.15285  ,  0.41475  , -0.3398   , -0.23043  ,  0.19069  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['dog']\n",
    "embeddings_index['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Implementation of Rapid Automatic Keyword Extraction algorithm.\n",
    "As described in the paper `Automatic keyword extraction from individual\n",
    "documents` by Stuart Rose, Dave Engel, Nick Cramer and Wendy Cowley.\n",
    "\"\"\"\n",
    "\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, groupby, product\n",
    "\n",
    "import nltk\n",
    "from enum import Enum\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "\n",
    "class Metric(Enum):\n",
    "    \"\"\"Different metrics that can be used for ranking.\"\"\"\n",
    "\n",
    "    DEGREE_TO_FREQUENCY_RATIO = 0  # Uses d(w)/f(w) as the metric\n",
    "    WORD_DEGREE = 1  # Uses d(w) alone as the metric\n",
    "    WORD_FREQUENCY = 2  # Uses f(w) alone as the metric\n",
    "\n",
    "\n",
    "class Rake(object):\n",
    "    \"\"\"Rapid Automatic Keyword Extraction Algorithm.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        stopwords=None,\n",
    "        punctuations=None,\n",
    "        language=\"english\",\n",
    "        ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO,\n",
    "        max_length=10000,\n",
    "        min_length=1,\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "        :param stopwords: List of Words to be ignored for keyword extraction.\n",
    "        :param punctuations: Punctuations to be ignored for keyword extraction.\n",
    "        :param language: Language to be used for stopwords\n",
    "        :param max_length: Maximum limit on the number of words in a phrase\n",
    "                           (Inclusive. Defaults to 100000)\n",
    "        :param min_length: Minimum limit on the number of words in a phrase\n",
    "                           (Inclusive. Defaults to 1)\n",
    "        \"\"\"\n",
    "        # By default use degree to frequency ratio as the metric.\n",
    "        if isinstance(ranking_metric, Metric):\n",
    "            self.metric = ranking_metric\n",
    "        else:\n",
    "            self.metric = Metric.DEGREE_TO_FREQUENCY_RATIO\n",
    "\n",
    "        # If stopwords not provided we use language stopwords by default.\n",
    "        self.stopwords = stopwords\n",
    "        if self.stopwords is None:\n",
    "            self.stopwords = nltk.corpus.stopwords.words(language)\n",
    "\n",
    "        # If punctuations are not provided we ignore all punctuation symbols.\n",
    "        self.punctuations = punctuations\n",
    "        if self.punctuations is None:\n",
    "            self.punctuations = string.punctuation\n",
    "\n",
    "        # All things which act as sentence breaks during keyword extraction.\n",
    "        self.to_ignore = set(chain(self.stopwords, self.punctuations))\n",
    "\n",
    "        # Assign min or max length to the attributes\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Stuff to be extracted from the provided text.\n",
    "        self.frequency_dist = None\n",
    "        self.degree = None\n",
    "        self.rank_list = None\n",
    "        self.ranked_phrases = None\n",
    "\n",
    "    def extract_keywords_from_text(self, text):\n",
    "        \"\"\"Method to extract keywords from the text provided.\n",
    "        :param text: Text to extract keywords from, provided as a string.\n",
    "        \"\"\"\n",
    "        sentences = nltk.tokenize.sent_tokenize(text)\n",
    "        self.extract_keywords_from_sentences(sentences)\n",
    "\n",
    "    def extract_keywords_from_sentences(self, sentences):\n",
    "        \"\"\"Method to extract keywords from the list of sentences provided.\n",
    "        :param sentences: Text to extraxt keywords from, provided as a list\n",
    "                          of strings, where each string is a sentence.\n",
    "        \"\"\"\n",
    "        phrase_list = self._generate_phrases(sentences)\n",
    "        self._build_frequency_dist(phrase_list)\n",
    "        self._build_word_co_occurance_graph(phrase_list)\n",
    "        self._build_ranklist(phrase_list)\n",
    "\n",
    "    def get_ranked_phrases(self):\n",
    "        \"\"\"Method to fetch ranked keyword strings.\n",
    "        :return: List of strings where each string represents an extracted\n",
    "                 keyword string.\n",
    "        \"\"\"\n",
    "        return self.ranked_phrases\n",
    "\n",
    "    def get_ranked_phrases_with_scores(self):\n",
    "        \"\"\"Method to fetch ranked keyword strings along with their scores.\n",
    "        :return: List of tuples where each tuple is formed of an extracted\n",
    "                 keyword string and its score. Ex: (5.68, 'Four Scoures')\n",
    "        \"\"\"\n",
    "        return self.rank_list\n",
    "\n",
    "    def get_word_frequency_distribution(self):\n",
    "        \"\"\"Method to fetch the word frequency distribution in the given text.\n",
    "        :return: Dictionary (defaultdict) of the format `word -> frequency`.\n",
    "        \"\"\"\n",
    "        return self.frequency_dist\n",
    "\n",
    "    def get_word_degrees(self):\n",
    "        \"\"\"Method to fetch the degree of words in the given text. Degree can be\n",
    "        defined as sum of co-occurances of the word with other words in the\n",
    "        given text.\n",
    "        :return: Dictionary (defaultdict) of the format `word -> degree`.\n",
    "        \"\"\"\n",
    "        return self.degree\n",
    "\n",
    "    def _build_frequency_dist(self, phrase_list):\n",
    "        \"\"\"Builds frequency distribution of the words in the given body of text.\n",
    "        :param phrase_list: List of List of strings where each sublist is a\n",
    "                            collection of words which form a contender phrase.\n",
    "        \"\"\"\n",
    "        self.frequency_dist = Counter(chain.from_iterable(phrase_list))\n",
    "\n",
    "    def _build_word_co_occurance_graph(self, phrase_list):\n",
    "        \"\"\"Builds the co-occurance graph of words in the given body of text to\n",
    "        compute degree of each word.\n",
    "        :param phrase_list: List of List of strings where each sublist is a\n",
    "                            collection of words which form a contender phrase.\n",
    "        \"\"\"\n",
    "        co_occurance_graph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        for phrase in phrase_list:\n",
    "            # For each phrase in the phrase list, count co-occurances of the\n",
    "            # word with other words in the phrase.\n",
    "            #\n",
    "            # Note: Keep the co-occurances graph as is, to help facilitate its\n",
    "            # use in other creative ways if required later.\n",
    "            for (word, coword) in product(phrase, phrase):\n",
    "                co_occurance_graph[word][coword] += 1\n",
    "        self.degree = defaultdict(lambda: 0)\n",
    "        for key in co_occurance_graph:\n",
    "            self.degree[key] = sum(co_occurance_graph[key].values())\n",
    "\n",
    "    def _build_ranklist(self, phrase_list):\n",
    "        \"\"\"Method to rank each contender phrase using the formula\n",
    "              phrase_score = sum of scores of words in the phrase.\n",
    "              word_score = d(w)/f(w) where d is degree and f is frequency.\n",
    "        :param phrase_list: List of List of strings where each sublist is a\n",
    "                            collection of words which form a contender phrase.\n",
    "        \"\"\"\n",
    "        self.rank_list = []\n",
    "        for phrase in phrase_list:\n",
    "            rank = 0.0\n",
    "            for word in phrase:\n",
    "                if self.metric == Metric.DEGREE_TO_FREQUENCY_RATIO:\n",
    "                    rank += 1.0 * self.degree[word] / self.frequency_dist[word]\n",
    "                elif self.metric == Metric.WORD_DEGREE:\n",
    "                    rank += 1.0 * self.degree[word]\n",
    "                else:\n",
    "                    rank += 1.0 * self.frequency_dist[word]\n",
    "            self.rank_list.append((rank, \" \".join(phrase)))\n",
    "        self.rank_list.sort(reverse=True)\n",
    "        self.ranked_phrases = [ph[1] for ph in self.rank_list]\n",
    "\n",
    "    def _generate_phrases(self, sentences):\n",
    "        \"\"\"Method to generate contender phrases given the sentences of the text\n",
    "        document.\n",
    "        :param sentences: List of strings where each string represents a\n",
    "                          sentence which forms the text.\n",
    "        :return: Set of string tuples where each tuple is a collection\n",
    "                 of words forming a contender phrase.\n",
    "        \"\"\"\n",
    "        phrase_list = set()\n",
    "        # Create contender phrases from sentences.\n",
    "        for sentence in sentences:\n",
    "            word_list = [word.lower() for word in wordpunct_tokenize(sentence)]\n",
    "            phrase_list.update(self._get_phrase_list_from_words(word_list))\n",
    "        return phrase_list\n",
    "\n",
    "    def _get_phrase_list_from_words(self, word_list):\n",
    "        \"\"\"Method to create contender phrases from the list of words that form\n",
    "        a sentence by dropping stopwords and punctuations and grouping the left\n",
    "        words into phrases. Only phrases in the given length range (both limits\n",
    "        inclusive) would be considered to build co-occurrence matrix. Ex:\n",
    "        Sentence: Red apples, are good in flavour.\n",
    "        List of words: ['red', 'apples', \",\", 'are', 'good', 'in', 'flavour']\n",
    "        List after dropping punctuations and stopwords.\n",
    "        List of words: ['red', 'apples', *, *, good, *, 'flavour']\n",
    "        List of phrases: [('red', 'apples'), ('good',), ('flavour',)]\n",
    "        List of phrases with a correct length:\n",
    "        For the range [1, 2]: [('red', 'apples'), ('good',), ('flavour',)]\n",
    "        For the range [1, 1]: [('good',), ('flavour',)]\n",
    "        For the range [2, 2]: [('red', 'apples')]\n",
    "        :param word_list: List of words which form a sentence when joined in\n",
    "                          the same order.\n",
    "        :return: List of contender phrases that are formed after dropping\n",
    "                 stopwords and punctuations.\n",
    "        \"\"\"\n",
    "        groups = groupby(word_list, lambda x: x not in self.to_ignore)\n",
    "        phrases = [tuple(group[1]) for group in groups if group[0]]\n",
    "        return list(\n",
    "            filter(\n",
    "                lambda x: self.min_length <= len(x) <= self.max_length, phrases\n",
    "            )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Helper functions\n",
    "###########################\n",
    "\n",
    "# Generate frequency dictionary of all words in all reviews (needed for TF-IDF)\n",
    "def generateCorpus(X):\n",
    "    all_words = defaultdict(int)\n",
    "    for k in range(0,len(X)):\n",
    "\t\twordList = re.sub(\"[^\\w]\", \" \",  X[k]).split() # Clean and split data\n",
    "\t\tfor words in wordList:\n",
    "\t\t\tall_words[words.lower()] += 1\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t, 'n') for t in word_tokenize(doc)]\n",
    "\n",
    "def TF_IDF_all(asin_dict):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", analyzer='word', lowercase = True, tokenizer = LemmaTokenizer(), \n",
    "\t\t\t\t ngram_range=(1, 2), min_df = 2, max_df = 0.8)\n",
    "\n",
    "    vec = vectorizer.fit_transform(list(asin_dict.values()))\n",
    "    word_map=vectorizer.get_feature_names()\n",
    "\n",
    "    return vec, word_map\n",
    "\n",
    "def TF_IDF_asin(asin, asin_dict, vectorizer, word_map, n):\n",
    "    index = asin_dict.keys().index(asin)\n",
    "    tf = vectorizer[index]\n",
    "    \n",
    "    keywords = []\n",
    "    for col in tf.nonzero()[1]:\n",
    "        keywords.append((tf[0, col],word_map[col]))\n",
    "\n",
    "    sorted_keywords = sorted(keywords, key=lambda t: t[1] * -1)\n",
    "\n",
    "    return sorted_keywords[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-a44819db4a1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0masin_dict1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0masin_mother_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0masin_dict1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masin_mother_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'asin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pos_reviews'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not str"
     ]
    }
   ],
   "source": [
    "#creating_asin_dict_for_TF-IDF\n",
    "asin_dict1={}\n",
    "for i in asin_mother_dict.keys():\n",
    "    asin_dict1[i]=(asin_mother_dict['asin']['pos_reviews'])\n",
    "\n",
    "    \n",
    "number_of_asins_on_which_keyword_separators_should_be_applied= 100\n",
    "counter=0\n",
    "rake_keywords_dict={}\n",
    "TFIDF_keywords_dict={}\n",
    "vec, map = TF_IDF_all(asin_dict)\n",
    "\n",
    "for i in asin_dict.keys():\n",
    "    if counter < number_of_asins_on_which_keyword_separators_should_be_applied:\n",
    "#         print asin_dict[i]\n",
    "        c = Rake()\n",
    "        c.extract_keywords_from_text(asin_dict[i])\n",
    "        rake_keywords_dict[i]= c.rank_list\n",
    "        counter+=1\n",
    "        \n",
    "        ###########################\n",
    "        # Run TF-IDF\n",
    "        ###########################\n",
    "#         #test_list = [0, 2, 3, 4, 8, 9, 12, 13, 14]\n",
    "#         test_list = [0]\n",
    "\n",
    "#         for i in test_list:\n",
    "#             my_asin = asin_dict.keys()[i]\n",
    "#             print my_asin\n",
    "#         print asin_title_map[i]\n",
    "\n",
    "        TFIDF_keywords_dict[i] = TF_IDF_asin(i, asin_dict, vec, map, 10)\n",
    "#             print keywords\n",
    "    else:\n",
    "        break\n",
    "        #     rl_list.append(c.rank_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# d = Rake()\n",
    "# d.extract_keywords_from_text('I love cats and cats')\n",
    "# print d.rank_list\n",
    "# print d.ranked_phrases\n",
    "print len(TFIDF_keywords_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:34: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:37: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:76: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:90: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:79: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "#converts reviews to embedding dict where key is the ASIN of a book\n",
    "count=0\n",
    "review_dict={}\n",
    "TFIDF_review_dict={}\n",
    "\n",
    "lower_cutoff_for_word_to_be_considered=1#model parameter\n",
    "TFIDF_lower_cutoff_for_word_to_be_considered=0.0#model parameter\n",
    "\n",
    "number_of_reviews_for_clustering=100#select how many reviews are being used for clustering, small number reduces computation time. \n",
    "\n",
    "max_word_per_review= 10#model parameter\n",
    "TFIDF_max_word_per_review= 10#model parameter\n",
    "\n",
    "\n",
    "if number_of_reviews_for_clustering > len(rake_keywords_dict.keys()):\n",
    "    print (\"number_of_reviews_for_clustering too large!!\")\n",
    "    print(\"number of reviews= \" + str(len(rake_keywords_dict.keys()))+ \" requested \" + str(number_of_reviews_for_clustering))\n",
    "else:\n",
    "    for i in rake_keywords_dict.keys():\n",
    "        normalization_factor=0\n",
    "#         print i\n",
    "        if count< number_of_reviews_for_clustering :\n",
    "            print count\n",
    "            count +=1\n",
    "    #         print rake_keywords_dict[i]\n",
    "            count_of_words_used=0\n",
    "#             print \"entering RAKE\"\n",
    "            for j in rake_keywords_dict[i]:\n",
    "#                 print \"entered RAKE\"\n",
    "                if count_of_words_used < max_word_per_review:\n",
    "                    count_of_words_used+=1\n",
    "                    if j[0] > lower_cutoff_for_word_to_be_considered:\n",
    "                        if j[1] not in embeddings_index.keys():\n",
    "                            for k in j[1].split():\n",
    "                #                 print (\"k=\" + k)\n",
    "                                if k in embeddings_index.keys():\n",
    "                                    if i not in review_dict.keys():\n",
    "\n",
    "                                        review_dict[i]= j[0]* embeddings_index[k] \n",
    "                                        normalization_factor+=1\n",
    "                                    else:\n",
    "                                        review_dict[i]+= j[0]* embeddings_index[k] \n",
    "                                        normalization_factor+=1\n",
    "        #                         else:\n",
    "        #                             print k\n",
    "                        else:\n",
    "                            if j[1] in embeddings_index.keys():\n",
    "                #                 print (\"j=\" + j[1])\n",
    "                    #             review_dict[i]+= rl_list[j][0]* embeddings_index[j][1]\n",
    "                                if i not in review_dict.keys():\n",
    "                                    review_dict[i]= j[0]* embeddings_index[j[1]]\n",
    "                                    normalization_factor+=1\n",
    "                                else:\n",
    "                                    review_dict[i]+= j[0]* embeddings_index[j[1]]\n",
    "                                    normalization_factor+=1\n",
    "        #                     else:\n",
    "        #                         print j[1]\n",
    "#             print (\"entering normalization\" + str(i))\n",
    "            if normalization_factor >0:\n",
    "#                     review_dict[i]= review_dict[i]/ normalization_factor\n",
    "                [z/normalization_factor for z in review_dict[i]]\n",
    "#             print (\"RAKE done..\")\n",
    "\n",
    "    #TF-IDF converts reviews to embedding dict where key is the ASIN of a book\n",
    "#             print (\"RAKE done..\")\n",
    "            TFIDF_normalization_factor=0\n",
    "            TFIDF_count_of_words_used=0\n",
    "#             print TFIDF_keywords_dict[i]\n",
    "#             print \"entering TFIDF\"\n",
    "            for j in TFIDF_keywords_dict[i]:\n",
    "                \n",
    "                if TFIDF_count_of_words_used < max_word_per_review:\n",
    "                    TFIDF_count_of_words_used+=1\n",
    "                    if j[0] > TFIDF_lower_cutoff_for_word_to_be_considered:\n",
    "                        if j[1] not in embeddings_index.keys():\n",
    "                            for k in j[1].split():\n",
    "#                                 print (\"k=\" + k)\n",
    "                                if k in embeddings_index.keys():\n",
    "#                                     print (\"k=\" + k)\n",
    "                                    if i not in TFIDF_review_dict.keys():\n",
    "                                        TFIDF_review_dict[i]= j[0]* embeddings_index[k] \n",
    "                                        TFIDF_normalization_factor+=1\n",
    "                                    else:\n",
    "                                        TFIDF_review_dict[i]+= j[0]* embeddings_index[k] \n",
    "                                        TFIDF_normalization_factor+=1\n",
    "        #                         else:\n",
    "        #                             print k\n",
    "                        else:\n",
    "                            if j[1] in embeddings_index.keys():\n",
    "#                                 print (\"j=\" + j[1])\n",
    "                                if i not in TFIDF_review_dict.keys():\n",
    "                                    TFIDF_review_dict[i]= j[0]* embeddings_index[j[1]]\n",
    "                                    TFIDF_normalization_factor+=1\n",
    "                                else:\n",
    "                                    TFIDF_review_dict[i]+= j[0]* embeddings_index[j[1]]\n",
    "                                    TFIDF_normalization_factor+=1\n",
    "        #                     else:\n",
    "        #                         print j[1]\n",
    "#             print (\"entering TFIDF Normalization\" + str(i))\n",
    "#             print TFIDF_review_dict\n",
    "            if TFIDF_normalization_factor >0:\n",
    "#                     review_dict[i]= review_dict[i]/ normalization_factor\n",
    "                [z/TFIDF_normalization_factor for z in TFIDF_review_dict[i]]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print len(TFIDF_review_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #TF-IDF converts reviews to embedding dict where key is the ASIN of a book\n",
    "# count=0\n",
    "# TFIDF_review_dict={}\n",
    "# number_of_reviews_for_clustering=5#select how many reviews are being used for clustering, small number reduces computation time. \n",
    "# if number_of_reviews_for_clustering > len(TFIDF_keywords_dict.keys()):\n",
    "#     print (\"number_of_reviews_for_clustering too large!!\")\n",
    "#     print(\"number of reviews= \" + str(len(rl_dict.keys()))+ \" requested \" + str(number_of_reviews_for_clustering))\n",
    "# else:\n",
    "#     max_word_per_review= 10#model parameter\n",
    "#     lower_cutoff_for_word_to_be_considered=0.0#model parameter\n",
    "#     for i in TFIDF_keywords_dict.keys():\n",
    "#         normalization_factor=0\n",
    "# #         print i\n",
    "#         if count< number_of_reviews_for_clustering :\n",
    "#             count +=1\n",
    "#             count_of_words_used=0\n",
    "# #             print TFIDF_keywords_dict[i]\n",
    "#             for j in TFIDF_keywords_dict[i]:\n",
    "#                 print j\n",
    "#                 if count_of_words_used < max_word_per_review:\n",
    "#                     count_of_words_used+=1\n",
    "#                     if j[0] > lower_cutoff_for_word_to_be_considered:\n",
    "#                         if j[1] not in embeddings_index.keys():\n",
    "#                             for k in j[1].split():\n",
    "#                                 print (\"k=\" + k)\n",
    "#                                 if k in embeddings_index.keys():\n",
    "#                                     if i not in TFIDF_review_dict.keys():\n",
    "#                                         TFIDF_review_dict[i]= j[0]* embeddings_index[k] \n",
    "#                                         normalization_factor+=1\n",
    "#                                     else:\n",
    "#                                         TFIDF_review_dict[i]+= j[0]* embeddings_index[k] \n",
    "#                                         normalization_factor+=1\n",
    "#         #                         else:\n",
    "#         #                             print k\n",
    "#                         else:\n",
    "#                             if j[1] in embeddings_index.keys():\n",
    "#                                 print (\"j=\" + j[1])\n",
    "#                                 if i not in TFIDF_review_dict.keys():\n",
    "#                                     TFIDF_review_dict[i]= j[0]* embeddings_index[j[1]]\n",
    "#                                     normalization_factor+=1\n",
    "#                                 else:\n",
    "#                                     TFIDF_review_dict[i]+= j[0]* embeddings_index[j[1]]\n",
    "#                                     normalization_factor+=1\n",
    "#         #                     else:\n",
    "#         #                         print j[1]\n",
    "#                 if normalization_factor >0:\n",
    "# #                     review_dict[i]= review_dict[i]/ normalization_factor\n",
    "#                     [z/normalization_factor for z in TFIDF_review_dict[i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =0\n",
    "for i in review_dict.keys():\n",
    "    if i not in TFIDF_review_dict.keys():\n",
    "        print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_dict_list=[]\n",
    "TFIDF_review_dict_list=[]\n",
    "for i in review_dict.keys():\n",
    "    review_dict_list.append(review_dict[i])\n",
    "    TFIDF_review_dict_list.append(TFIDF_review_dict[i])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 9 0 0 0 6 2 6 0 0 7 2 0 0 5 9 6 2 6 3 0 0 9 0 6 0 0 6 6 6 6 0 6 6 6 0 0\n",
      " 6 7 6 0 7 0 0 9 1 9 9 9 8 0 9 9 6 6 6 0 0 0 0 9 7 0 0 6 0 6 9 0 6 6 6 0 6\n",
      " 0 6 9 0 6 9 7 0 0 0 6 6 0 6 6 0 6 2 6 7 4 0 7 0 1 0]\n",
      "[4 7 2 4 7 7 6 7 3 0 5 5 7 9 5 5 2 0 2 0 0 0 5 5 0 0 4 0 5 4 0 0 0 4 0 0 7\n",
      " 0 1 5 5 5 5 7 7 0 0 7 0 5 3 0 5 0 0 7 9 3 3 4 5 4 9 0 2 7 7 5 9 0 0 7 2 5\n",
      " 3 0 7 0 5 5 5 9 8 7 0 0 3 0 0 3 2 5 4 5 7 0 5 9 5 4]\n"
     ]
    }
   ],
   "source": [
    "#holy K-means\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit_predict(review_dict_list)\n",
    "TFIDF_kmeans= KMeans(n_clusters=10, random_state=0).fit_predict(TFIDF_review_dict_list)\n",
    "print kmeans\n",
    "print TFIDF_kmeans\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAKE_Clustered Titles\n",
      "TFIDF_Clustered Titles\n"
     ]
    }
   ],
   "source": [
    "#matches the embeddings of review to a title\n",
    "clusteredTitles= collections.defaultdict(list)\n",
    "for i in range(0, len(kmeans)):\n",
    "    for asin_value, word2VecList in review_dict.items():\n",
    "        if (word2VecList == review_dict_list[i]).all():\n",
    "#             print asin_value\n",
    "            clusteredTitles[kmeans[i]].append(asin_title_map[asin_value])\n",
    "            break\n",
    "print (\"RAKE_Clustered Titles\")\n",
    "# print clusteredTitles\n",
    "\n",
    "TFIDF_clusteredTitles= collections.defaultdict(list)\n",
    "for i in range(0, len(TFIDF_kmeans)):\n",
    "    for asin_value, word2VecList in TFIDF_review_dict.items():\n",
    "        if (word2VecList == TFIDF_review_dict_list[i]).all():\n",
    "#             print asin_value\n",
    "            TFIDF_clusteredTitles[TFIDF_kmeans[i]].append(asin_title_map[asin_value])\n",
    "            break\n",
    "print (\"TFIDF_Clustered Titles\")\n",
    "# print TFIDF_clusteredTitles\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAKE_Cluster Number: 0['Tweezerman  Nose Hair Clipper', 'NuGo Organic Nutrition Bar, Dark Double Chocolate, 1.76-Ounce Bars (Pack of 12)', 'Firehouse Moustache Wax, Light', 'Now Foods Slippery Elm 400mg, Capsules, 100-Count', 'Trojan Bareskin Lubricated Condoms 10pc (3 Pack)', \"Dental Floss Smart Floss Dr. Tung's 30 yd String\", 'derma e Psorzema, Natural Relief for Scaling, Flaking, and Itching, 4 Ounce (113 g)', 'Soft Scrub with Bleach Cleanser, 24-Ounce (Pack of 3)', 'Trojan Vibrating Tri-phoria', 'PowerFactor AA Alkaline Battery 24 Pack', 'Diastix Reagent Strips for Urinalysis to test urine Glucose - 50 Strips', 'Dymatize Nutrition BCAA Complex 5050 Powder, 10.7 Ounce', 'Truform 8810,  Compression Stockings, Anti-Embolism, Thigh High, Closed-Toe, 18 mmHg, Beige, X-Large', 'Mighty Bright 87022 Wallet Magnifier w/ LED Light, Silver', 'Aveeno 1% Hydrocortisone Anti-Itch Cream, Maximum Strength, 1-Ounce Tubes (Pack of 4)', 'Terry Naturally Curamin, 60 Caps', 'Source Naturals Essential Enzymes 500 Mg Vegetarian Capsules, 120-Count', \"Delsym Children's Cough Suppressant, Orange, 5 Ounce\", 'Smartek Battery Operated Fabric Clothes Shaver - Black', 'Auric Blends - Sandalwood Vanilla Body Oil', 'Crest Complete Multi-Benefit Whitening + Scope Outlast Long Lasting Mint Flavor Toothpaste 5.8 Oz', 'Jabon Zote Blanco Finas Escamas Para Lavadora (Laundry Flakes for Washiing Machines), 17.6 Oz., (Pack of 1)', 'Ox Bile 125 mg 180 Capsules', 'TOUCH N BROW Razor (3 Pcs)', 'Drive Medical Toilet Safety Frame, White', 'Bona Stone Tile and Laminate Floor Cleaner Refill, 128-Ounce', '1300 mg Acai  Juice Extreem TM (3 Bottles) New Stronger Potency HIGH POTENCY ACAI Berry Natural Nutrition, Energy 3 Months 180 Caps, 1300 Mg, Acai Juice Extreme', 'Master Series Jizz Water Based Lube, Semen Scented, 8.5 Ounce', 'New Chapter Organics, Berry Green, Vegetarian Capsules, 90-Count', 'Kleenex 2-Ply White Facial Tissue, 10-Pack Bundle (230 tissues per box)', 'Waterpik Waterflosser Ultra And Waterpik Cordless Plus Combo Pack Includes 12 Accessory Tips &amp; Travel Case', 'Cleanstream XL Lubricant Launcher', 'Epilady EP-803-17 Face-Epil Facial and Sensitive Areas Epilator', 'Nivea Lip Butter Loose Tin, Raspberry Rose Kiss, 0.59 Ounce', 'Dynamic Health Nopal Gold - 100% Pure Organic Certified Nopal Juice 32oz, 32 Fluid Ounce', 'Sally Hansen Clinical Lipcare Collection Peptide Lip Line Treatment, 0.42 Ounce', 'Boericke &amp; Tafel Arniflora Arnica Natural Topical Pain Reliever Gel, Maximum Strength 2.75oz (pack of 2)', 'Faultless Tiny Kit (Reusable Douche) - 16 Fl Oz Capacity', 'Diamond Edge Professional Straight Razor Includes 5 double edge blades']\n",
      "RAKE_Cluster Number: 1[\"Degree Clinical Protection, Men's Anti-perspirant and Deodorant, Clean, 1.7-Ounce Stick (Pack of 2)\", 'Ecotones Sound + Sleep Machine, Model ASM1002']\n",
      "RAKE_Cluster Number: 2['Natural Factors Wild Alaskan Salmon Oil 1000mg Softgels, 180-Count', 'Energizer Recharge Value Battery Charger', '2013 Hammer Nutrition Complex Carbohydrate Energy Gel', 'Duracell 80226938 CopperTop Alkaline-Manganese Dioxide Battery with Duralock Power Preserve Technology, AA Size, 1.5V, (Pack of 14)']\n",
      "RAKE_Cluster Number: 3['Febreze Thai Dragon Fruit Deodorizing Powder Endorsed by BISSELL, 32 ounces']\n",
      "RAKE_Cluster Number: 4['Move Free Advanced Glucosamine Nutritional Supplements Gummies, 70 Count']\n",
      "RAKE_Cluster Number: 5['Air Wick Scented Oil Triple Refill Relaxation, Apple Cinnamon Medley, 0.67 Ounce Containers']\n",
      "RAKE_Cluster Number: 6['Nivea Dry Comfort Deodorant Antiperspirant Roll-on 50 ml', 'Ice Pack (6x10 Size)', 'Sleep Essentials 60 Caps', 'Blue Moon Timer and Stopwatch', 'URS Urinalysis Strips For Ketone - 100 ea', 'BISSELL Flip-!t Select Hard Floor Cleaner with Heat, 7340', \"One a Day Women's Vitacraves Gummies, 100 Count\", 'Pumpkin Patch Premium Grade Fragrance Oils - 10ml - Scented Oil', 'Teeth Whitening Light Kit with Photo Initiator gel of 44%', '3M Medipore H Soft Cloth Surgical Tape - 4&quot; wide by 10 yards', 'SimplyBeautiful 100% Pure Badger Brush with Chrome Handle - *Special Price*', 'Omron ComFit Cuff H-CL22, Replacement Cuff for HEM-711DLX, HEM-780 and HEM-790IT.', 'Blush G-spot Slimline Vibrator, Pink', \"Doctor's Best 20 Mg Lutein Esters Vegetarian Capsules, 120 Count\", 'Zud Multi-Purpose Cream Cleanser: 19 OZ', \"Microfiber Cloth, Soft, Scratch-free Cleaning and Dusting Towel-for Automotive, Electronics, Health and Personal, and Home and Kitchen, Keep Your Windows Clean in Your House and Car. Keep Your Car's Showroom Shine, Bonus Pk. Satisfaction Guarantee.\", 'Sony 377 - SR626SW Button Cell Battery', '4th Generation Dr Mom LED POCKET Otoscope with Protective Foam Lined Case', 'Garcinia Cambogia Extract Pure Plus - Clinically Proven Natural Weight Loss, Appetite Suppressant and Fat Burning - Pure 50% HCA Extract - 1000 mg Per Serving Per 2 Capsules - As Seen on TV - 100% Satisfaction Guarantee - Plus A FREE BONUS REPORT On How To Accelerate Your Weight Loss!', 'Drive Medical Walker Basket, White', 'Nicotine Transdermal System Patch, Stop Smoking Aid, 7 mg, Step 3, 14 patches', 'Now Foods Candida Support, 90 Vcaps (Pack of 2)', 'Clotrimazole, AF Antifungal AthleteS Foot Topical Solution 1 Percent (Generic Lotrimin) - 10 Ml', 'Bausch &amp; Lomb Sight Savers Premoistened Lens Cleaning Tissues - 100 Count, 2 pk.', 'Hitachi Magic Wand Massager Only', 'Duracell Coppertop MN1500 AA Batteries , 100 Pack Count', 'Le Couvent des Minimes Eau des Minimes Everyday Deodorant with Alum Stone, 1.7 oz', 'Nature Made Vitamin D3 2000 IU, Value Size, 220-Count', 'Mueller Adjustable Lumbar Back Brace, Black, Plus Size, 1-Count Package', 'Boiron Homeopathic Medicine Oscillococcinum for Flu- Box of 6x 0.04oz Doses (Pack of 2 boxes)', None]\n",
      "RAKE_Cluster Number: 7['Telebrands Earth to skin- Heeltastic intensive heel therapy 2oz 57g', 'Professional Water Resistant Heavy Duty Steel Nose Trimmer with LED light. Backed by a Lifetime Guarantee', 'RockTape Kinesiology Tape for  Athletes (2-Inch x 16.4-Feet)', 'Sinus Plumber Horseradish and Pepper Nasal Spray - Natural Allergy Relief', \"Dr. Scholl's Men's Over-the-calf Compression 1 Pair Sock\", 'Charmin Ultra Strong, 12 Mega Rolls (Pack of 4)', 'Glide Pro-Health For Life Smooth Mint Floss 35 Meters']\n",
      "RAKE_Cluster Number: 8['Charmin Ultra Soft, Mega Rolls, 6 Count Packs (Pack of 3) 18 Total Rolls']\n",
      "RAKE_Cluster Number: 9['Orajel Severe Toothache Pain Relief Cream', 'Wholesomes Sweetners - Organic Molasses, 16 oz liquid', \"Old Spice High Endurance, Original Scent Men's Anti-Perspirant &amp; Deodorant 3 Oz (Pack of 6)\", \"Nature's Way Scullcap Herb, 100 Capsule\", \"Nature's Best Zero Carb Isopure, Strawberries and Cream, 7.5-Pound Tub\", 'Twinlab Super E-Complex 400 IU, 250 Softgels', 'PLANETARY HERBALS Triphala Gold Ayurvedic Herbal Supplement, 1000 Mg, 120 Count', 'Gold Bond Triple Action Medicated Body Powder -- 10 oz', 'Saffron Extract- Appetite Suppressant | 100% Pure Premium Saffron Extract | 88.5mg - 90 Veggie Capsules |1 Pill Per Serving', 'Clearblue Advanced Pregnancy Test With Weeks Estimator 3 Count', 'Thera-Band Light Flex Bar', 'Premier Nutrition High Protein Shake, Vanilla,  11 oz.,18 Count', 'Amberen Healthy Choice for Menopause, 60-Count']\n",
      "TFIDF_Cluster Number: 0[\"Dental Floss Smart Floss Dr. Tung's 30 yd String\", '2013 Hammer Nutrition Complex Carbohydrate Energy Gel', 'Febreze Thai Dragon Fruit Deodorizing Powder Endorsed by BISSELL, 32 ounces', 'Trojan Vibrating Tri-phoria', 'PowerFactor AA Alkaline Battery 24 Pack', 'URS Urinalysis Strips For Ketone - 100 ea', 'Dymatize Nutrition BCAA Complex 5050 Powder, 10.7 Ounce', 'BISSELL Flip-!t Select Hard Floor Cleaner with Heat, 7340', 'Teeth Whitening Light Kit with Photo Initiator gel of 44%', 'Mighty Bright 87022 Wallet Magnifier w/ LED Light, Silver', '3M Medipore H Soft Cloth Surgical Tape - 4&quot; wide by 10 yards', 'Omron ComFit Cuff H-CL22, Replacement Cuff for HEM-711DLX, HEM-780 and HEM-790IT.', 'Aveeno 1% Hydrocortisone Anti-Itch Cream, Maximum Strength, 1-Ounce Tubes (Pack of 4)', 'Blush G-spot Slimline Vibrator, Pink', \"Degree Clinical Protection, Men's Anti-perspirant and Deodorant, Clean, 1.7-Ounce Stick (Pack of 2)\", \"Nature's Best Zero Carb Isopure, Strawberries and Cream, 7.5-Pound Tub\", 'PLANETARY HERBALS Triphala Gold Ayurvedic Herbal Supplement, 1000 Mg, 120 Count', 'Gold Bond Triple Action Medicated Body Powder -- 10 oz', 'Zud Multi-Purpose Cream Cleanser: 19 OZ', \"Microfiber Cloth, Soft, Scratch-free Cleaning and Dusting Towel-for Automotive, Electronics, Health and Personal, and Home and Kitchen, Keep Your Windows Clean in Your House and Car. Keep Your Car's Showroom Shine, Bonus Pk. Satisfaction Guarantee.\", 'Bona Stone Tile and Laminate Floor Cleaner Refill, 128-Ounce', 'Drive Medical Walker Basket, White', 'Nicotine Transdermal System Patch, Stop Smoking Aid, 7 mg, Step 3, 14 patches', 'Bausch &amp; Lomb Sight Savers Premoistened Lens Cleaning Tissues - 100 Count, 2 pk.', 'Waterpik Waterflosser Ultra And Waterpik Cordless Plus Combo Pack Includes 12 Accessory Tips &amp; Travel Case', 'Duracell Coppertop MN1500 AA Batteries , 100 Pack Count', 'Le Couvent des Minimes Eau des Minimes Everyday Deodorant with Alum Stone, 1.7 oz', 'Nature Made Vitamin D3 2000 IU, Value Size, 220-Count', 'Mueller Adjustable Lumbar Back Brace, Black, Plus Size, 1-Count Package', 'Boericke &amp; Tafel Arniflora Arnica Natural Topical Pain Reliever Gel, Maximum Strength 2.75oz (pack of 2)']\n",
      "TFIDF_Cluster Number: 1['Professional Water Resistant Heavy Duty Steel Nose Trimmer with LED light. Backed by a Lifetime Guarantee']\n",
      "TFIDF_Cluster Number: 2['NuGo Organic Nutrition Bar, Dark Double Chocolate, 1.76-Ounce Bars (Pack of 12)', 'Sleep Essentials 60 Caps', 'Blue Moon Timer and Stopwatch', '4th Generation Dr Mom LED POCKET Otoscope with Protective Foam Lined Case', 'New Chapter Organics, Berry Green, Vegetarian Capsules, 90-Count', 'Boiron Homeopathic Medicine Oscillococcinum for Flu- Box of 6x 0.04oz Doses (Pack of 2 boxes)']\n",
      "TFIDF_Cluster Number: 3['Trojan Bareskin Lubricated Condoms 10pc (3 Pack)', 'Auric Blends - Sandalwood Vanilla Body Oil', 'Jabon Zote Blanco Finas Escamas Para Lavadora (Laundry Flakes for Washiing Machines), 17.6 Oz., (Pack of 1)', 'Ox Bile 125 mg 180 Capsules', 'Kleenex 2-Ply White Facial Tissue, 10-Pack Bundle (230 tissues per box)', 'Dynamic Health Nopal Gold - 100% Pure Organic Certified Nopal Juice 32oz, 32 Fluid Ounce', 'Sally Hansen Clinical Lipcare Collection Peptide Lip Line Treatment, 0.42 Ounce']\n",
      "TFIDF_Cluster Number: 4['Tweezerman  Nose Hair Clipper', 'Firehouse Moustache Wax, Light', 'Truform 8810,  Compression Stockings, Anti-Embolism, Thigh High, Closed-Toe, 18 mmHg, Beige, X-Large', 'Pumpkin Patch Premium Grade Fragrance Oils - 10ml - Scented Oil', 'SimplyBeautiful 100% Pure Badger Brush with Chrome Handle - *Special Price*', 'TOUCH N BROW Razor (3 Pcs)', 'Sinus Plumber Horseradish and Pepper Nasal Spray - Natural Allergy Relief', None, 'Diamond Edge Professional Straight Razor Includes 5 double edge blades']\n",
      "TFIDF_Cluster Number: 5['Telebrands Earth to skin- Heeltastic intensive heel therapy 2oz 57g', 'Energizer Recharge Value Battery Charger', 'Air Wick Scented Oil Triple Refill Relaxation, Apple Cinnamon Medley, 0.67 Ounce Containers', 'Wholesomes Sweetners - Organic Molasses, 16 oz liquid', \"Old Spice High Endurance, Original Scent Men's Anti-Perspirant &amp; Deodorant 3 Oz (Pack of 6)\", 'Diastix Reagent Strips for Urinalysis to test urine Glucose - 50 Strips', \"One a Day Women's Vitacraves Gummies, 100 Count\", \"Doctor's Best 20 Mg Lutein Esters Vegetarian Capsules, 120 Count\", 'Source Naturals Essential Enzymes 500 Mg Vegetarian Capsules, 120-Count', 'RockTape Kinesiology Tape for  Athletes (2-Inch x 16.4-Feet)', \"Delsym Children's Cough Suppressant, Orange, 5 Ounce\", 'Charmin Ultra Soft, Mega Rolls, 6 Count Packs (Pack of 3) 18 Total Rolls', 'Saffron Extract- Appetite Suppressant | 100% Pure Premium Saffron Extract | 88.5mg - 90 Veggie Capsules |1 Pill Per Serving', 'Clearblue Advanced Pregnancy Test With Weeks Estimator 3 Count', 'Thera-Band Light Flex Bar', 'Clotrimazole, AF Antifungal AthleteS Foot Topical Solution 1 Percent (Generic Lotrimin) - 10 Ml', 'Hitachi Magic Wand Massager Only', 'Amberen Healthy Choice for Menopause, 60-Count', \"Dr. Scholl's Men's Over-the-calf Compression 1 Pair Sock\", 'Duracell 80226938 CopperTop Alkaline-Manganese Dioxide Battery with Duralock Power Preserve Technology, AA Size, 1.5V, (Pack of 14)', 'Charmin Ultra Strong, 12 Mega Rolls (Pack of 4)', 'Glide Pro-Health For Life Smooth Mint Floss 35 Meters', 'Ecotones Sound + Sleep Machine, Model ASM1002']\n",
      "TFIDF_Cluster Number: 6['Natural Factors Wild Alaskan Salmon Oil 1000mg Softgels, 180-Count']\n",
      "TFIDF_Cluster Number: 7['Orajel Severe Toothache Pain Relief Cream', 'Now Foods Slippery Elm 400mg, Capsules, 100-Count', 'Nivea Dry Comfort Deodorant Antiperspirant Roll-on 50 ml', 'Ice Pack (6x10 Size)', 'derma e Psorzema, Natural Relief for Scaling, Flaking, and Itching, 4 Ounce (113 g)', 'Terry Naturally Curamin, 60 Caps', 'Smartek Battery Operated Fabric Clothes Shaver - Black', \"Nature's Way Scullcap Herb, 100 Capsule\", 'Twinlab Super E-Complex 400 IU, 250 Softgels', 'Sony 377 - SR626SW Button Cell Battery', '1300 mg Acai  Juice Extreem TM (3 Bottles) New Stronger Potency HIGH POTENCY ACAI Berry Natural Nutrition, Energy 3 Months 180 Caps, 1300 Mg, Acai Juice Extreme', 'Garcinia Cambogia Extract Pure Plus - Clinically Proven Natural Weight Loss, Appetite Suppressant and Fat Burning - Pure 50% HCA Extract - 1000 mg Per Serving Per 2 Capsules - As Seen on TV - 100% Satisfaction Guarantee - Plus A FREE BONUS REPORT On How To Accelerate Your Weight Loss!', 'Now Foods Candida Support, 90 Vcaps (Pack of 2)', 'Premier Nutrition High Protein Shake, Vanilla,  11 oz.,18 Count', 'Nivea Lip Butter Loose Tin, Raspberry Rose Kiss, 0.59 Ounce', 'Move Free Advanced Glucosamine Nutritional Supplements Gummies, 70 Count']\n",
      "TFIDF_Cluster Number: 8['Epilady EP-803-17 Face-Epil Facial and Sensitive Areas Epilator']\n",
      "TFIDF_Cluster Number: 9['Soft Scrub with Bleach Cleanser, 24-Ounce (Pack of 3)', 'Crest Complete Multi-Benefit Whitening + Scope Outlast Long Lasting Mint Flavor Toothpaste 5.8 Oz', 'Drive Medical Toilet Safety Frame, White', 'Master Series Jizz Water Based Lube, Semen Scented, 8.5 Ounce', 'Cleanstream XL Lubricant Launcher', 'Faultless Tiny Kit (Reusable Douche) - 16 Fl Oz Capacity']\n"
     ]
    }
   ],
   "source": [
    "for i in clusteredTitles.keys():\n",
    "    print \"RAKE_Cluster Number: \" + str(i)+ str(clusteredTitles[i])\n",
    "\n",
    "for i in TFIDF_clusteredTitles.keys():\n",
    "    print \"TFIDF_Cluster Number: \" + str(i)+ str(TFIDF_clusteredTitles[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B00AZOOW6O\n"
     ]
    }
   ],
   "source": [
    "# #computation for example in the poster\n",
    "# name='Duracell 80226938 CopperTop Alkaline-Manganese Dioxide Battery with Duralock Power Preserve Technology, AA Size, 1.5V, (Pack of 14)'\n",
    "# for j in asin_title_map.keys():\n",
    "#     if asin_title_map[j] == name:\n",
    "#         print j\n",
    "#         break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #computation for example in the poster\n",
    "# pos_reviews_B00AZOOW6O = ''\n",
    "# neg_reviews_B00AZOOW6O = ''\n",
    "\n",
    "# for i in data:\n",
    "#     if i['asin'] == 'B00AZOOW6O':\n",
    "# #     asin_dict[i['asin']].append(i['reviewText'])\n",
    "#         if i['overall'] >= 4:\n",
    "#             pos_reviews_B00AZOOW6O+= i['reviewText']\n",
    "\n",
    "#         else:\n",
    "#             neg_reviews_B00AZOOW6O+=i['reviewText']\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_reviews_B00AZOOW6O= I buy Duracell based on previous experience.  The ProCell batteries are what I've used for years and I have had no issues at all.  I bought standard Coppertops because they were on sale and I see no difference thus far.  They seem to last an appropriate amount of time in remotes and kids toys, which is my main usage.  Given that the ProCell batteries are usually cheaper and just as effective, I have to rate the Coppertops 4 stars.  They work great, but not better than a cheaper alternative.Duracell batteries are about as good as any other high quality battery. One aspect I like of these &#34;Duralock&#34; batteries is that they have a shelf life of about 10 years. I can remember years ago a battery would have a shelf life of about 1 year!  Although I have a bunch of alkalines, when they're gone I'm going rechargables! Rechargables such as Eneloop are high quality and VERY cost effective despite their intial $$$.These Alkaline-Manganese Dioxide batteries seem very well made, arrived promptly with excellent packaging.  Though I've only used a few of them, they seem just fine forflash lights, wireless keep board and mouse for my computer.I ordered these batteries cheaper than what I could find locally, especially with free shipping. You can never have too many batteries around the house for remotes and grand kids toys.These batteries are made in USA and have an expiration date of 2019. I bought some AAA batteries right before this purchase and the seller said &#34;made in USA.&#34; When the batteries arrived, seven were dented with some having battery acid coming out in the package and were made in China. The seller said it was due to hurricane Sandy. I sympathize with the company and the folks that lost so much in the hurricane, but the seller of these AA batteries was in the same area and I had no problem with them nor their advertising. I will purchase these batteries again the Amazon.com and Duracell.I purchased these batteries a week ago and was satisfied for the value.  Getting the 28 pack last year was a far better value, however getting less batteries for the same price appears to the cost of doing business. Regarding freshness, these were very fresh batteries with 12 years of guaranteed life through 2022.These batteries is ultra strong and last a long time. Very good quality to get for any electronics. good job duracelli have used duracell batteries for years, as you all know they are the best batteries that are out thereI love the Duracell batteries above all brands.  These are fresh and supply power at needed levels. I highly recommend.All you have to do is plan a little ahead and buy batteries here. Those impulse buys at the check out counter at retailers is where you get hit. These are great for the price to have on hand at home.I love that you can add batteries to your Amazon order! It's so annoying to have to run to the store every time batteries are needed.i HAVE ONLY USED A FEW OF THEM IN MY FLAMELESS CANDLES, AND THEY SEEM FINE SO FAR.  I THINK THE PRICE IS VERY GOOD.The batteries worked fine, as expected.  They didn't seem to drain really fast or have any issues.  Used them in kids toys and remotes, worked just fine.great deal on these batteries and with the storm season approaching always need x-tras - stock up now at this low price\n",
      "neg_reviews_B00AZOOW6O =Using these in my 6 year old Kodak C180 digital camera. Traditional Copper Tops last longer. What's the big deal beyond a bigger price?It pains me to write this as most of my purchases with Amazon are 5 stars, but these batteries appear to be fraud!  First the unlabeled box made to hold 20 was opened and there are oily surfaced 14 batteries.  Then I checked the labels and two were blistering and had to be pitched immediately.  Thinking these were the ones leaking the oil, I took a rag and cleaned to oil off the rest of the batteries.  I found two more batteries with different labels from the rest, and a closer look found that their dates were marked 2015 and the rest were marked 2019.  None of the the advertised 10 yrs shelf life.  I tested each one and found all of them dead.  I then checked the checker by testing them inside an old mouse and it confirmed they were dead on arrival.  I then put 4 of them inside a recharger made for alk batteries, and as they started to recharge, all leaked oil and 2 bubbled a brown substance which ruined a perfectly good charger.  By now, do you think I would trust these batteries inside a children's toy or any of my electronic gear.  NO Way!!!  Cost to return prohibits return, so this warning to those of you who read this to not trust this vendor &#34;utrusted&#34; who appears to be recycling batteries or mfg fraud copies.  As they say, never trust someone who says trust me, don't trust this vendor.  I trusted and got scammed.After answering a question about how sure someone could be about these, I took mine to a known electronics retailer and had these tested.  Due to an error by Amazon's Customer Service, I ended up receiving too many boxes of these batteries.  In each box of 14, there were at least 4 batteries that were no good.  I cancelled my Subscribe & Save subscription.  Since I have more than I ordered, and Amazon refunded the error amount charged to me, it'd cost me more than it's worth to return them, but I will not be reordering them.\n",
      "I buy Duracell based on previous experience.  The ProCell batteries are what I've used for years and I have had no issues at all.  I bought standard Coppertops because they were on sale and I see no difference thus far.  They seem to last an appropriate amount of time in remotes and kids toys, which is my main usage.  Given that the ProCell batteries are usually cheaper and just as effective, I have to rate the Coppertops 4 stars.  They work great, but not better than a cheaper alternative. Duracell batteries are about as good as any other high quality battery. One aspect I like of these &#34;Duralock&#34; batteries is that they have a shelf life of about 10 years. I can remember years ago a battery would have a shelf life of about 1 year!  Although I have a bunch of alkalines, when they're gone I'm going rechargables! Rechargables such as Eneloop are high quality and VERY cost effective despite their intial $$$. These Alkaline-Manganese Dioxide batteries seem very well made, arrived promptly with excellent packaging.  Though I've only used a few of them, they seem just fine forflash lights, wireless keep board and mouse for my computer. Using these in my 6 year old Kodak C180 digital camera. Traditional Copper Tops last longer. What's the big deal beyond a bigger price? It pains me to write this as most of my purchases with Amazon are 5 stars, but these batteries appear to be fraud!  First the unlabeled box made to hold 20 was opened and there are oily surfaced 14 batteries.  Then I checked the labels and two were blistering and had to be pitched immediately.  Thinking these were the ones leaking the oil, I took a rag and cleaned to oil off the rest of the batteries.  I found two more batteries with different labels from the rest, and a closer look found that their dates were marked 2015 and the rest were marked 2019.  None of the the advertised 10 yrs shelf life.  I tested each one and found all of them dead.  I then checked the checker by testing them inside an old mouse and it confirmed they were dead on arrival.  I then put 4 of them inside a recharger made for alk batteries, and as they started to recharge, all leaked oil and 2 bubbled a brown substance which ruined a perfectly good charger.  By now, do you think I would trust these batteries inside a children's toy or any of my electronic gear.  NO Way!!!  Cost to return prohibits return, so this warning to those of you who read this to not trust this vendor &#34;utrusted&#34; who appears to be recycling batteries or mfg fraud copies.  As they say, never trust someone who says trust me, don't trust this vendor.  I trusted and got scammed. I ordered these batteries cheaper than what I could find locally, especially with free shipping. You can never have too many batteries around the house for remotes and grand kids toys.These batteries are made in USA and have an expiration date of 2019. I bought some AAA batteries right before this purchase and the seller said &#34;made in USA.&#34; When the batteries arrived, seven were dented with some having battery acid coming out in the package and were made in China. The seller said it was due to hurricane Sandy. I sympathize with the company and the folks that lost so much in the hurricane, but the seller of these AA batteries was in the same area and I had no problem with them nor their advertising. I will purchase these batteries again the Amazon.com and Duracell. I purchased these batteries a week ago and was satisfied for the value.  Getting the 28 pack last year was a far better value, however getting less batteries for the same price appears to the cost of doing business. Regarding freshness, these were very fresh batteries with 12 years of guaranteed life through 2022. After answering a question about how sure someone could be about these, I took mine to a known electronics retailer and had these tested.  Due to an error by Amazon's Customer Service, I ended up receiving too many boxes of these batteries.  In each box of 14, there were at least 4 batteries that were no good.  I cancelled my Subscribe & Save subscription.  Since I have more than I ordered, and Amazon refunded the error amount charged to me, it'd cost me more than it's worth to return them, but I will not be reordering them. These batteries is ultra strong and last a long time. Very good quality to get for any electronics. good job duracell i have used duracell batteries for years, as you all know they are the best batteries that are out there I love the Duracell batteries above all brands.  These are fresh and supply power at needed levels. I highly recommend. All you have to do is plan a little ahead and buy batteries here. Those impulse buys at the check out counter at retailers is where you get hit. These are great for the price to have on hand at home. I love that you can add batteries to your Amazon order! It's so annoying to have to run to the store every time batteries are needed. i HAVE ONLY USED A FEW OF THEM IN MY FLAMELESS CANDLES, AND THEY SEEM FINE SO FAR.  I THINK THE PRICE IS VERY GOOD. The batteries worked fine, as expected.  They didn't seem to drain really fast or have any issues.  Used them in kids toys and remotes, worked just fine. great deal on these batteries and with the storm season approaching always need x-tras - stock up now at this low price\n"
     ]
    }
   ],
   "source": [
    "# #computation for example in the poster\n",
    "# print \"pos_reviews_B00AZOOW6O= \" + pos_reviews_B00AZOOW6O \n",
    "# print \"neg_reviews_B00AZOOW6O =\" + neg_reviews_B00AZOOW6O\n",
    "# print asin_dict['B00AZOOW6O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #computation for example in the poster\n",
    "# # rl_list=[]\n",
    "# # number_of_asins_on_which_keyword_separators_should_be_applied= 100\n",
    "# # counter=0\n",
    "# # rl_dict={}\n",
    "# # TFIDF_keywords_dict={}\n",
    "# # vec, map = TF_IDF_all(asin_dict)\n",
    "# c = Rake()\n",
    "# c.extract_keywords_from_text(pos_reviews_B00AZOOW6O)\n",
    "# RAKE_pos_reviews_B00AZOOW6O= c.rank_list\n",
    "# print RAKE_pos_reviews_B00AZOOW6O\n",
    "\n",
    "# c = Rake()\n",
    "# c.extract_keywords_from_text(neg_reviews_B00AZOOW6O)\n",
    "# RAKE_neg_reviews_B00AZOOW6O= c.rank_list\n",
    "# print RAKE_neg_reviews_B00AZOOW6O\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'reviewerID': u'AHKSURW85PJUE', u'asin': u'159985130X', u'reviewerName': u'AZ buyer \"AZ buyer\"', u'helpful': [1, 1], u'reviewText': u'I would recommend this for a travel magnifier for the occasional reading.I had read on another review about a magnifier having a problem with the light coming on. I did find that this one appeared to be DOA out of the box. But, after opening & shutting the viewer to turn on & off the light, the light began to come on. After several times of doing this, the light appears to be coming on all the time.It is small, but for taking it someplace & reading things like a menu in a dark corner of a restaurant, this is great.', u'overall': 4.0, u'summary': u'Small & may need to encourage battery', u'unixReviewTime': 1329523200, u'reviewTime': u'02 18, 2012'}\n"
     ]
    }
   ],
   "source": [
    "print data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
