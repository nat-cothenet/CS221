{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CS221_11292018_0824 includes a RAKE class that applies RAKE on the input file\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "import gzip\n",
    "import statistics as stat\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading data\n",
    "        \n",
    "#This block reads 5-core dataset\n",
    "# number_of_data_points_to_read= 10000 #Select number of data points to read\n",
    "counter= 0\n",
    "data=[]\n",
    "path= '/Users/srishti/Desktop/reviews_Health_and_Personal_Care_5.json'\n",
    "with open(path) as f:\n",
    "    for line in f:\n",
    "#         if counter <number_of_data_points_to_read:\n",
    "        data.append(json.loads(line))\n",
    "        counter+=1\n",
    "#         else: \n",
    "#             break\n",
    "\n",
    "X=[] #reivews\n",
    "Y=[] #ratings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_mother_dict= collections.defaultdict(dict)\n",
    "asin_pos_reviews_dict = collections.defaultdict(list)\n",
    "temp_dict_all_parameters_known={}\n",
    "# counter= len(data)\n",
    "counter=0\n",
    "#Number of data_points_to_select\n",
    "number_of_data_points_to_select= len(data)\n",
    "for i in data:\n",
    "    if counter < number_of_data_points_to_select:\n",
    "        counter+=1\n",
    "        if i['asin'] not in asin_mother_dict.keys():\n",
    "            temp_dict={}\n",
    "            temp_dict['pos_reviews']=\"\"\n",
    "            temp_dict['neg_reviews']=\"\"\n",
    "            asin_mother_dict[i['asin']]=temp_dict\n",
    "            asin_pos_reviews_dict[i['asin']]= \"\" \n",
    "\n",
    "    #     asin_pos_reviews_dict[i['asin']].append(i['reviewText'])\n",
    "        if i['overall'] >= 4:\n",
    "            temp_dict={}\n",
    "            temp_dict['pos_reviews']= asin_mother_dict[i['asin']]['pos_reviews']+ \" \"+ i['reviewText']\n",
    "            asin_mother_dict[i['asin']].update(temp_dict) \n",
    "            asin_pos_reviews_dict[i['asin']]= asin_mother_dict[i['asin']]\n",
    "\n",
    "        else:\n",
    "            temp_dict={}\n",
    "            temp_dict['neg_reviews']= asin_mother_dict[i['asin']]['neg_reviews']+ \" \"+ i['reviewText']\n",
    "            asin_mother_dict[i['asin']].update(temp_dict) \n",
    "    #         asin_mother_dict[i['asin']]['neg_reviews'] = asin_mother_dict[i['asin']]['neg_reviews']+ \" \"+ i['reviewText']\n",
    "\n",
    "    for i in asin_mother_dict.keys():\n",
    "        if asin_mother_dict[i]['pos_reviews']==\"\":\n",
    "            asin_mother_dict[i]['all_param_known']='False'\n",
    "        else:\n",
    "            asin_mother_dict[i]['all_param_known']='True'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3812028492 {'all_param_known': 'True', 'neg_reviews': u' Real nice build to this razor. It\\'s hard to beat the German quality. However, I believe this razor is not aggressive enough. I actually got a  better shave with my $12.00 Lord. I just don\\'t get that close shave feeling, even after two passes. Maybe I should try a third pass, but I don\\'t think it\\'s going to matter. I\\'ve used both Feather and the Gillette 7 O\\'clock Sharp Edge blades. The new feather worked better this morning. I\\'ll try it for a few days more and see how it goes. I had this razor for 5 years.  Finally, the handle separated from the head of the razor while closing the razor head with the knob at the bottom.  There is a screw head that comes out of the handle, which the razor head attaches by surrounding the screw head with with plating.  I wasn\\'t abusing the razor or over-tightening it.  I was hoping to get more \"bang for the buck\" out of this razor.  I think the longer handle of this razor enables more force, while closing the razor.Since then, I\\'ve purchased the 34C (shorter handle).  I dropped the razor in the shower and, again, the head separated from the handle.  Is this a common design for plated DE razors?  Is it the chrome plating that\\'s susceptible to this?  I don\\'t know.I also found the spiral lines on the handle didn\\'t help get a firm grip on the razor.  My hands are occasionally soapy in the shower (surprise!) and every once in a while it would get away from me.  I don\\'t find this to be an issue with the 34C\\'s different design.The Merkur 38C is lackluster and I\\'ll be looking at other non-Merkur razors for my next purchase.', 'pos_reviews': u\" great quality, feels very good in my hand, very good weight, non slippery handle. it's a very good buy, cant wrong with this one. 34c is also very good, but shorter handle. Make no mistake this is a sturdy long handled razor. Feels substantial in your hand and the finish makes it easy to hold onto when wet. Easy to keep clean with the two piece design. Plating was beautiful and very good quality razor. My first safety razor, and I'll probably never have to buy another.My list of Amazon-acquired shaving supplies:Deluxe Stainless Steel Safety Razor and Shaving Brush Stand from Super Safety RazorsMerkur Classic Barbor Pole Long Handle Safety Razor #38 + 10 Free DE Razor BladesTruefitt & Hill Ultimate Comfort Pre-Shave Oil, 2 oz.Truefitt & Hill 1805 Aftershave BalmTweezerman  Men's Shaving BrushTruefitt & Hill 1805 Shave Cream JarOsma Styptic Pencil, Hemo StopFeather Hi-Stainless Platinum Double Edge Razor Blades 30 Ct100 Astra Superior Premium Platinum Double Edge Razor Blades I made the jump to a safety razor because of the cost of disposable cartridges, but also because I thought i would enjoy to ritual of shaving this way.  All I can say is good choice by me!  It absolutely takes some getting used to (buy a Dabon by Pinaud styptic pencil at the same time) but once you get a handle on it you won't regret the switch.  This type of shaving produces a closer shave than i ever got with the most expensive multi-blade cartridges and now I'm down to one or two nicks a week, mostly because I forget to take my time.  Start out angling the razor by raising your wrist in order to avoid slicing yourself up and as you get used to it gradually lower the wrist and you'll see the true effectiveness of the razor.  This particular model has plenty of heft.  It looks and feels like the quality tool that it is.  Get yourself a badger hair bursh and some Taylor of Bond Street shaving cream, crank up the hot water and enjoy the gentlemanly ritual of a quality shave.  Pick up some after shave cream in order to soothe your skin because if you've been shaving with cartridges your face won't be used to the closeness of this shave.  Also, keep a few cartridges around for those mornings when you're running late.  Rush through a shave with this (or any other safety razor) and you'll pay the price. Being an extra large (and then some) person, I elected to get a razor with a bigger heavier handle, and this delivers.  Growing up appreciating German quality, I also found that to be appealing (it kinda delivers there).  All in all I'm happy, though not ecstatic about the purchase.Pros:Big and heavy as advertisedWell built at a functional levelLess aggressive head is forgiving for novicesIncludes 1 bladeCons:May be too big and heavy for someDetails are a bit rough for $50More advanced users may want to upgrade to a more aggressive head (if needed)Could include at least a 10 pack of blades for the priceI don't regret my purchase, though my next purchase likely would be a different model.  It's very forgiving for my novice skills, and that's a huge plus, but one that I'll outgrow (I hope!).  I really wish the details on the finish would wow me with craftsmanship, and it doesn't.  It's just average, and honestly less than I'd expect for $50. This is my first safety Razor.  it's worked very well so far.  It has a great weight to it and feels good in your hand.  It took me a long time to convince myself to spend $50+ on a razor, but it has been a good one so far.  The long handle is better for my because I have bear paws... Awesome razor. Went to this from a Mach 5 blade. SOOOO Much happier with this then any disposable blade I have ever used. The shave is incredibly close and smooth. The weight of the blade feels nice in you hand. Replacement blades are less then $0.50 each and last just as long as the disposables. Plus it's manly. Can't recommend enough.\"}\n",
      "B00009R7VT {'all_param_known': 'True', 'neg_reviews': u\" I have some old slides and they are dusty and have spots.  I read in reviews that this cloth was recommended.  I was very disappointed when it scratched one of my slides though it didn't scratch all of them.  I would have to say to be careful and try on one that is not important. Even then I would be hesitant.  Back to the drawing board as far as my slides.  I will just use this cloth to clean my computer screens.\", 'pos_reviews': u' I had one of these back in the day, and they are without a doubt the best negative/slide cleaning cloths on the market. The anti-static properties are highly effective at repelling (well, stopping the attraction of) dust particles, and I rarely found a scratch on my negative after using my Tiger Cloth.Paired with a good negative cleaner, you can ensure that your negatives will be properly ready for the best scans possible. My only complaint? The color is hideous!At this price, you really can\\'t afford to NOT use one! This is not the original thin, silky feeling Tiger Cloth. I\\'m giving it 5 stars because the original is excellent.I talked with tech support and they said I was right that this model was thicker and heavier. Tech support said they had so many complaints that they were reverting back to the original cloth specifications. I used the cloth today and it shed microfibers profusely, which made it totally useless for cleaning my camera lens. I\\'m going to try and wash it and use it again, but I do not like the soft, thick feeling it has. For one, it doesn\\'t compress like the original and takes up more space. Two, it feels like a towel, not a lens cloth.At the time of this writing, the original Tiger Cloth should be in stock, but make sure before you buy. Note that the original Tiger Cloth is about the best lens cleaning cloth you can buy. I suppose there are many reasons that one might want an anti-static microfiber cloth. Here is mine: I play a lot of vinyl records and I clear them periodically, but they don\\'t need a deep cleaning every single time I pull them out. This is a nice way to get the little dust fragments off of the surface before playing the record. Audio companies make fancy versions of this same thing and they cost more than twice as much, but why not save some money? As a dedicated amateur photographer and photo restorer, I\\'m finding infinite uses for this cloth.  It is anti-static and lint-free. If it was nothing else, these two attributes already make it worth its price.It\\'s safe to use in delicate electronic equipment in which static might cause damage (such as the inside of your camera or your computer).It cleans whatever you want cleaned without scratching or leaving any lint or residue behind. This is perfect for cleaning your film slides or negatives as well as your photos prior to scanning, with or without the use of a photographic solution. It\\'s thin enough that you don\\'t lose the \"feel\" of what you\\'re handling and thick enough to do the job. (When using photographic solutions, you still need to use gloves, so the easy-handling quality of the cloth is an added bonus). It\\'s hard to describe the texture, but it\\'s sort of silky but absorbent, it\\'s lightweight and soft, and thinner than a microfiber cloth.I\\'ve used it on my camera lenses, the live-view screen on my digital camera, and the viewfinder as well. If you have a glossy digital camera body, it\\'s also perfect for that.It is listed as being safe to clean: computer monitors, plasma tv screens, DVD\\'s, copier glass, plastics, circuit boards, and other industrial, photographic, and electronic equipment. I haven\\'t tried it on all of these, but on those products on which I have tried it (primarily photography-related uses) I have honestly found it to be the best material I\\'ve ever used for that purpose.The 10\" x 18\" size might be a little more than you need for some uses. I\\'ve cut both of mine into two 10\" x 9\" pieces since the surfaces I need to clean are usually not too large and a smaller cloth gives me better control. The cloth can be hand-washed and machine dried on \"warm\" setting. It will, eventually, need replacement but it seems to last a reasonable amount of time. I\\'ve washed mine a few times already and don\\'t notice any deterioration.Combined with the Kinetronics Precision Cleaning Solution it does the best job possible cleaning your flat scanner\\'s glass bed. Note that, if you buy the scanner cleaner \"kit\" you already get a cloth with it, so that might be your best option.Personally, I think it has so many uses that you can\\'t have too many. I like to keep the ones I\\'ve used with a solution separately from the ones I\\'ve used dry; the ones I use to clean the inside of the camera separately from the ones I\\'ve used to clean negatives; the ones I\\'ve used to clean moldy old photographs from the ones I\\'ve used to clean undamaged ones...you get the idea.I\\'m happy to have found a product that does so many things so well and it will become a staple that I\\'ll keep as a permanent tool in my work flow.'}\n"
     ]
    }
   ],
   "source": [
    "counter=0\n",
    "for i in asin_mother_dict.keys():\n",
    "    if counter <2:\n",
    "        print i+ \" \"+ str(asin_mother_dict[i])\n",
    "        counter+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asin_mother_dict_copy= asin_mother_dict.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asin_mother_dict= asin_mother_dict_copy.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block reads meta data\n",
    "#reading meta data\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "meta_data = []\n",
    "# meta_path= '/Users/srishti/Desktop/meta_Books.json.gz'\n",
    "meta_path= '/Users/srishti/Desktop/meta_Health_and_Personal_Care.json.gz'\n",
    "counter= 1\n",
    "\n",
    "\n",
    "for line in parse(meta_path):\n",
    "    meta_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "asin_title_map = collections.defaultdict(list)\n",
    "all_asins_set= set(asin_mother_dict.keys())\n",
    "\n",
    "for i in meta_data:\n",
    "    temp_dict={}\n",
    "    temp_dict_all_parameters_known={}\n",
    "    if i['asin'] in all_asins_set:\n",
    "        all_asins_set.remove(i['asin'])\n",
    "        #Getting Titles\n",
    "        if 'title' in i.keys():\n",
    "            temp_dict['title']= i['title']\n",
    "#             print i['asin']\n",
    "#             print asin_mother_dict[i['asin']]\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "    #         asin_title_map[i['asin']] = i['title']\n",
    "\n",
    "        else:\n",
    "            temp_dict['title']= None\n",
    "            temp_dict['all_param_known']='False'\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "            asin_title_map[i['asin']] = None\n",
    "\n",
    "        #Adding also_bought, also_viewed, bought_together and buy_after_viewing\n",
    "        temp_dict={}\n",
    "        temp_dict_all_parameters_known={}\n",
    "        if 'related' in i.keys():\n",
    "            if 'also_bought' in i['related']:\n",
    "                temp_dict['also_bought']= i['related']['also_bought']\n",
    "            else:\n",
    "                temp_dict['also_bought']= None\n",
    "\n",
    "            if 'also_viewed' in i['related']:\n",
    "                temp_dict['also_viewed']= i['related']['also_viewed']\n",
    "            else:\n",
    "                temp_dict['also_viewed']= None\n",
    "\n",
    "            if 'bought_together' in i['related']:\n",
    "                temp_dict['bought_together']= i['related']['bought_together']\n",
    "            else:\n",
    "                temp_dict['bought_together']= None\n",
    "\n",
    "            if 'buy_after_viewing' in i['related']:\n",
    "                temp_dict['buy_after_viewing']= i['related']['buy_after_viewing']\n",
    "            else:\n",
    "                temp_dict['buy_after_viewing']= None\n",
    "\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "        else:\n",
    "            temp_dict['all_param_known']='False'\n",
    "            temp_dict['also_viewed']= None\n",
    "            temp_dict['also_bought']= None\n",
    "            temp_dict['bought_together']= None\n",
    "            temp_dict['buy_after_viewing']= None\n",
    "\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "\n",
    "        #Adding categories\n",
    "        #NOTE HEALTH & PERSONAL CARE IS A CATEGORY FOR ALL PRODUCT, NEED TO ADDRESS THAT\n",
    "        temp_dict={}\n",
    "        temp_dict_all_parameters_known={}\n",
    "        if 'categories' in i.keys():\n",
    "            temp_dict['categories']= i['categories'][0]\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "        else:\n",
    "            temp_dict['categories']= None\n",
    "            temp_dict['all_param_known']='False'\n",
    "            asin_mother_dict[i['asin']].update(temp_dict)\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3812028492\n",
      "{'buy_after_viewing': None, 'title': 'Merkur Long Handled Safety Razor 38c', 'pos_reviews': u\" great quality, feels very good in my hand, very good weight, non slippery handle. it's a very good buy, cant wrong with this one. 34c is also very good, but shorter handle. Make no mistake this is a sturdy long handled razor. Feels substantial in your hand and the finish makes it easy to hold onto when wet. Easy to keep clean with the two piece design. Plating was beautiful and very good quality razor. My first safety razor, and I'll probably never have to buy another.My list of Amazon-acquired shaving supplies:Deluxe Stainless Steel Safety Razor and Shaving Brush Stand from Super Safety RazorsMerkur Classic Barbor Pole Long Handle Safety Razor #38 + 10 Free DE Razor BladesTruefitt & Hill Ultimate Comfort Pre-Shave Oil, 2 oz.Truefitt & Hill 1805 Aftershave BalmTweezerman  Men's Shaving BrushTruefitt & Hill 1805 Shave Cream JarOsma Styptic Pencil, Hemo StopFeather Hi-Stainless Platinum Double Edge Razor Blades 30 Ct100 Astra Superior Premium Platinum Double Edge Razor Blades I made the jump to a safety razor because of the cost of disposable cartridges, but also because I thought i would enjoy to ritual of shaving this way.  All I can say is good choice by me!  It absolutely takes some getting used to (buy a Dabon by Pinaud styptic pencil at the same time) but once you get a handle on it you won't regret the switch.  This type of shaving produces a closer shave than i ever got with the most expensive multi-blade cartridges and now I'm down to one or two nicks a week, mostly because I forget to take my time.  Start out angling the razor by raising your wrist in order to avoid slicing yourself up and as you get used to it gradually lower the wrist and you'll see the true effectiveness of the razor.  This particular model has plenty of heft.  It looks and feels like the quality tool that it is.  Get yourself a badger hair bursh and some Taylor of Bond Street shaving cream, crank up the hot water and enjoy the gentlemanly ritual of a quality shave.  Pick up some after shave cream in order to soothe your skin because if you've been shaving with cartridges your face won't be used to the closeness of this shave.  Also, keep a few cartridges around for those mornings when you're running late.  Rush through a shave with this (or any other safety razor) and you'll pay the price. Being an extra large (and then some) person, I elected to get a razor with a bigger heavier handle, and this delivers.  Growing up appreciating German quality, I also found that to be appealing (it kinda delivers there).  All in all I'm happy, though not ecstatic about the purchase.Pros:Big and heavy as advertisedWell built at a functional levelLess aggressive head is forgiving for novicesIncludes 1 bladeCons:May be too big and heavy for someDetails are a bit rough for $50More advanced users may want to upgrade to a more aggressive head (if needed)Could include at least a 10 pack of blades for the priceI don't regret my purchase, though my next purchase likely would be a different model.  It's very forgiving for my novice skills, and that's a huge plus, but one that I'll outgrow (I hope!).  I really wish the details on the finish would wow me with craftsmanship, and it doesn't.  It's just average, and honestly less than I'd expect for $50. This is my first safety Razor.  it's worked very well so far.  It has a great weight to it and feels good in your hand.  It took me a long time to convince myself to spend $50+ on a razor, but it has been a good one so far.  The long handle is better for my because I have bear paws... Awesome razor. Went to this from a Mach 5 blade. SOOOO Much happier with this then any disposable blade I have ever used. The shave is incredibly close and smooth. The weight of the blade feels nice in you hand. Replacement blades are less then $0.50 each and last just as long as the disposables. Plus it's manly. Can't recommend enough.\", 'also_bought': ['B000850C1Y', 'B001XURHNY', 'B001G5FOLI', 'B000JPMHWK', 'B003WR3QSG', 'B00A3LVMMQ', 'B00HLOQZ0M', 'B0022R947O', 'B001O8NCI4', 'B0007MW2ZW', 'B000NCMMBA', 'B003BHCAGQ', 'B0038KA5RC', 'B001QY8QXM', 'B00BCNCKBQ', 'B0077LAJT2', 'B000G647Y8', 'B004U6P0NW', 'B004B6V24M', 'B007OL72B8', 'B00AGG3MUI', 'B00BFCV5JM', 'B000QYEK88', 'B00AGG3MNU', 'B00DWG863M', 'B00DWGEY1K', 'B002Z85VJK', 'B00E5QJC04', 'B0072AWXC4', 'B001XURHES', 'B004UIZFQW', 'B001VCUQE6', 'B004SGKMA0', 'B004L4EING', 'B007F6FCS2', 'B00AGG3MRG', 'B001PZEHIK', 'B001PEZNA2', 'B0006M56BK', 'B002HTI2TI', 'B002UB3BF8', 'B00GNEIVGW', 'B004RWTQTS', 'B00DWG7Z0C', 'B00FIN8R22', 'B000GCSFWW', 'B003LY2PG4', 'B00158Q99W', 'B008WG9IP2', 'B001PZCJC6', 'B00KH3N80E', 'B0030HQ6NU', 'B000RIAD0W', 'B002Z6YXIW', 'B00408QC3C', 'B002ET1ATU', 'B001PZ8W5O', 'B001G5HZCY', 'B002QI45Z0', 'B002AN5RWQ', 'B001PEXND6', 'B003BQWA80', 'B003BWQX46', 'B004V59IYE', 'B0026DWB7Y', 'B00FA35K02', 'B002CLH5MG', 'B005A96JBK', 'B000K8FUFM', 'B001OXTHJC', 'B004RQ4ACW', 'B00A9TZMQE', 'B001XURHBQ', 'B004NEHR28', 'B003GLOKC4', 'B005M3B8W4', 'B00FYKVM0S', 'B00375BUQ8', 'B00DWG833K', 'B0076ZW0ZU', 'B00B1TKY8C', 'B002IM0RP6', 'B005E1D9M6', 'B003VZPTAC', 'B002Y1451S', 'B003B3WEZW', 'B0068RXP0G', 'B000VL52GU', 'B00DWG7Z7A', 'B00A6IKO98', 'B0029U0YZK', 'B001XURHAC', 'B000MXGMHU', 'B0014E2Q8K', 'B001B8E77M', 'B0026MRA1W', 'B004L48O36', 'B002Z7UQ1Y'], 'neg_reviews': u' Real nice build to this razor. It\\'s hard to beat the German quality. However, I believe this razor is not aggressive enough. I actually got a  better shave with my $12.00 Lord. I just don\\'t get that close shave feeling, even after two passes. Maybe I should try a third pass, but I don\\'t think it\\'s going to matter. I\\'ve used both Feather and the Gillette 7 O\\'clock Sharp Edge blades. The new feather worked better this morning. I\\'ll try it for a few days more and see how it goes. I had this razor for 5 years.  Finally, the handle separated from the head of the razor while closing the razor head with the knob at the bottom.  There is a screw head that comes out of the handle, which the razor head attaches by surrounding the screw head with with plating.  I wasn\\'t abusing the razor or over-tightening it.  I was hoping to get more \"bang for the buck\" out of this razor.  I think the longer handle of this razor enables more force, while closing the razor.Since then, I\\'ve purchased the 34C (shorter handle).  I dropped the razor in the shower and, again, the head separated from the handle.  Is this a common design for plated DE razors?  Is it the chrome plating that\\'s susceptible to this?  I don\\'t know.I also found the spiral lines on the handle didn\\'t help get a firm grip on the razor.  My hands are occasionally soapy in the shower (surprise!) and every once in a while it would get away from me.  I don\\'t find this to be an issue with the 34C\\'s different design.The Merkur 38C is lackluster and I\\'ll be looking at other non-Merkur razors for my next purchase.', 'also_viewed': ['B000RJUZMW', 'B000NL0T1G', 'B000QYEK88', 'B004V59IYE', 'B00DON576M', 'B002FC7B2G', 'B000VXMMZW', 'B004C6DTZ6', 'B002CLH5MG', 'B005RZDEEC', 'B0047ACVUQ', 'B001LYAQCG', 'B002RBF1PO', 'B003NTHWLU', 'B002A8JO48', 'B0010Y1KYA', 'B003LW4L2W', 'B001LYAQ9E', 'B00375BUQ8', 'B004VBISXU', 'B000NHSAVQ', 'B000JPKLUU', 'B0037X9DFU', 'B00CPCLAL4', 'B00I9J6J2M', 'B0068RYVEK', 'B009L4YE5S', 'B00BJ9W144', 'B000JPMMPM', 'B000VCD4D2', 'B00EIRVNHU', 'B0047A6WLA', 'B002QTX8WU', 'B0022R966S', 'B001LY7GEM', 'B001AKFXN8', 'B003SISMJC', 'B00333GSLQ', 'B00C9M506G', 'B002CLL85Q', 'B0017ZHZZ0', 'B001B4TSP2', 'B004E5GV82', 'B003LW4LJK', 'B002P9KRHU', 'B0009IGAM0', 'B007OL72B8', 'B001XURHNY', 'B00374F5CY', 'B0022R74BC', 'B00A3LVMMQ', 'B0017QSZY4', 'B000JPMHWK', 'B001G5FOLI', 'B001LY5WZ2', 'B002A8JO1Q', 'B000QTYQ6Y', 'B003WR3QSG', 'B000LY2AKI', 'B001GHQRA8'], 'bought_together': ['B001XURHNY', 'B003WR3QSG'], 'all_param_known': 'True', 'categories': ['Health & Personal Care', 'Personal Care', 'Shaving & Hair Removal', 'Manual Shaving']}\n",
      "B00009R7VT\n",
      "{'buy_after_viewing': ['B00017LSPI', 'B0000AE6AG', 'B0001M6K24', 'B002OEBMRU'], 'title': 'Kinetronics Anti-static Microfiber Cloth, 10x18-Inch Tiger Cloth', 'pos_reviews': u' I had one of these back in the day, and they are without a doubt the best negative/slide cleaning cloths on the market. The anti-static properties are highly effective at repelling (well, stopping the attraction of) dust particles, and I rarely found a scratch on my negative after using my Tiger Cloth.Paired with a good negative cleaner, you can ensure that your negatives will be properly ready for the best scans possible. My only complaint? The color is hideous!At this price, you really can\\'t afford to NOT use one! This is not the original thin, silky feeling Tiger Cloth. I\\'m giving it 5 stars because the original is excellent.I talked with tech support and they said I was right that this model was thicker and heavier. Tech support said they had so many complaints that they were reverting back to the original cloth specifications. I used the cloth today and it shed microfibers profusely, which made it totally useless for cleaning my camera lens. I\\'m going to try and wash it and use it again, but I do not like the soft, thick feeling it has. For one, it doesn\\'t compress like the original and takes up more space. Two, it feels like a towel, not a lens cloth.At the time of this writing, the original Tiger Cloth should be in stock, but make sure before you buy. Note that the original Tiger Cloth is about the best lens cleaning cloth you can buy. I suppose there are many reasons that one might want an anti-static microfiber cloth. Here is mine: I play a lot of vinyl records and I clear them periodically, but they don\\'t need a deep cleaning every single time I pull them out. This is a nice way to get the little dust fragments off of the surface before playing the record. Audio companies make fancy versions of this same thing and they cost more than twice as much, but why not save some money? As a dedicated amateur photographer and photo restorer, I\\'m finding infinite uses for this cloth.  It is anti-static and lint-free. If it was nothing else, these two attributes already make it worth its price.It\\'s safe to use in delicate electronic equipment in which static might cause damage (such as the inside of your camera or your computer).It cleans whatever you want cleaned without scratching or leaving any lint or residue behind. This is perfect for cleaning your film slides or negatives as well as your photos prior to scanning, with or without the use of a photographic solution. It\\'s thin enough that you don\\'t lose the \"feel\" of what you\\'re handling and thick enough to do the job. (When using photographic solutions, you still need to use gloves, so the easy-handling quality of the cloth is an added bonus). It\\'s hard to describe the texture, but it\\'s sort of silky but absorbent, it\\'s lightweight and soft, and thinner than a microfiber cloth.I\\'ve used it on my camera lenses, the live-view screen on my digital camera, and the viewfinder as well. If you have a glossy digital camera body, it\\'s also perfect for that.It is listed as being safe to clean: computer monitors, plasma tv screens, DVD\\'s, copier glass, plastics, circuit boards, and other industrial, photographic, and electronic equipment. I haven\\'t tried it on all of these, but on those products on which I have tried it (primarily photography-related uses) I have honestly found it to be the best material I\\'ve ever used for that purpose.The 10\" x 18\" size might be a little more than you need for some uses. I\\'ve cut both of mine into two 10\" x 9\" pieces since the surfaces I need to clean are usually not too large and a smaller cloth gives me better control. The cloth can be hand-washed and machine dried on \"warm\" setting. It will, eventually, need replacement but it seems to last a reasonable amount of time. I\\'ve washed mine a few times already and don\\'t notice any deterioration.Combined with the Kinetronics Precision Cleaning Solution it does the best job possible cleaning your flat scanner\\'s glass bed. Note that, if you buy the scanner cleaner \"kit\" you already get a cloth with it, so that might be your best option.Personally, I think it has so many uses that you can\\'t have too many. I like to keep the ones I\\'ve used with a solution separately from the ones I\\'ve used dry; the ones I use to clean the inside of the camera separately from the ones I\\'ve used to clean negatives; the ones I\\'ve used to clean moldy old photographs from the ones I\\'ve used to clean undamaged ones...you get the idea.I\\'m happy to have found a product that does so many things so well and it will become a staple that I\\'ll keep as a permanent tool in my work flow.', 'also_bought': ['B00009R72R', 'B0036WBWP6', 'B0001M6K24', 'B00009R90P', 'B00014K2XU', 'B004XOLGG6', 'B00017LSPI', 'B004IP5H6K', 'B0000AE67U', 'B000EZU0WE', 'B002OEBMRU', 'B00009R7XP', 'B00009R8W7', 'B0000AE6AG', 'B0000AE67W', 'B00009R90O', 'B00FZ6VV20', 'B000CAHCQS', 'B004UT0T5S', 'B00009R90A', 'B00009V3C4', 'B005K08G1U', 'B00016SGHM', 'B00009V3C5', 'B00009R90W', 'B00009USUN', 'B000I2JI3K', 'B0036W9XUW', 'B00009R7XR', 'B007O9JP10', 'B00009R8ZW', 'B00009R90M', '193395230X', 'B00880BB9S', 'B00009R90I', 'B0058M3RC8', 'B000I2JI3A', 'B0036W9XUC', 'B00AGV7TQG', 'B00009R90C', '7576580800', 'B0010PCL5Q', '0316373052', 'B004PHB1S4', 'B005MJ8BSW', 'B000EZY19W', 'B00009R6YZ', 'B00ICOB78K', 'B00FRS479A', 'B008Q0EDDQ', 'B0009E6OEI', 'B00009V39M', 'B005KMPSQ4', 'B00009R8ZO', '0240812085', 'B00009XVF2', 'B00GIDADP0', 'B0000AE67V', '0136031870', 'B000GOZRNK', 'B00009R8YO', 'B0027A5E34', 'B000CHK7OA', 'B0036WFLBM', 'B000E0RCMU', 'B0000BZMIH', 'B00B5WIJ7I', 'B00BR54JHM', 'B0073HCE3O', 'B0009Z0WD6', '1597111961', 'B00009V3CG', 'B001JQVPBM', 'B00009R8ZU', '0205066402', 'B004FNRTGG', 'B004RIC11C', 'B00009R8YH', 'B00548T4FK', 'B007TIYZKC', 'B000L9OIQC', 'B001GO5TU0', 'B00004ZCA0', 'B00009R8ZX', 'B004O7HIRI', 'B0045KB06Y', 'B00009V3CH', 'B0000ALLYO', 'B00009R8W8', 'B000L9OIQ2', 'B00009R68V', 'B004LGIXOY', '1597111724', 'B000V4OSMQ', 'B00009V39N', 'B004BNF7JG', 'B002P4XREA', 'B00FKLQ0LW'], 'neg_reviews': u\" I have some old slides and they are dusty and have spots.  I read in reviews that this cloth was recommended.  I was very disappointed when it scratched one of my slides though it didn't scratch all of them.  I would have to say to be careful and try on one that is not important. Even then I would be hesitant.  Back to the drawing board as far as my slides.  I will just use this cloth to clean my computer screens.\", 'also_viewed': None, 'bought_together': ['B0001M6K24', 'B00017LSPI'], 'all_param_known': 'True', 'categories': ['Health & Personal Care', 'Household Supplies', 'Household Cleaning', 'Cloths & Wipes']}\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in asin_mother_dict.keys():\n",
    "    if count < 2:\n",
    "        print i\n",
    "        print asin_mother_dict[i]\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_mother_dict_copy= asin_mother_dict.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "466\n"
     ]
    }
   ],
   "source": [
    "for i in asin_mother_dict.keys():\n",
    "    if asin_mother_dict[i]['all_param_known']!= 'True':\n",
    "        del(asin_mother_dict[i])\n",
    "print len(asin_mother_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "embeddings_index = dict()\n",
    "f = open('/Users/srishti/Google Drive/000_7th Quarter/CS221/Project/untouched_data/glove.6B/glove.6B.50d.txt')\n",
    "for line in f:# first element of each line is the word and remaining elements are numerical represenation of each line \n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs# creates a dict of words and numerical representation of that word\n",
    "f.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02648  ,  0.33737  ,  0.065667 , -0.11609  ,  0.41651  ,\n",
       "       -0.21142  , -0.69582  ,  0.2822   , -0.36077  , -0.13822  ,\n",
       "        0.012094 ,  0.086227 , -0.84638  ,  0.057195 ,  1.1582   ,\n",
       "        0.14703  , -0.0049197, -0.24899  , -0.96014  , -0.3038   ,\n",
       "        0.23972  ,  0.21058  ,  0.40608  ,  0.17789  ,  0.55253  ,\n",
       "       -1.6357   , -0.17784  , -0.45222  ,  0.45805  ,  0.14239  ,\n",
       "        3.7087   ,  0.40289  , -0.4083   , -0.29304  ,  0.030857 ,\n",
       "       -0.15361  ,  0.10607  ,  0.63397  ,  0.12397  , -0.25349  ,\n",
       "       -0.10344  ,  0.0069768, -0.17328  ,  0.35536  , -0.46369  ,\n",
       "        0.15285  ,  0.41475  , -0.3398   , -0.23043  ,  0.19069  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index['dog']\n",
    "embeddings_index['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Implementation of Rapid Automatic Keyword Extraction algorithm.\n",
    "As described in the paper `Automatic keyword extraction from individual\n",
    "documents` by Stuart Rose, Dave Engel, Nick Cramer and Wendy Cowley.\n",
    "\"\"\"\n",
    "\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, groupby, product\n",
    "\n",
    "import nltk\n",
    "from enum import Enum\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "\n",
    "class Metric(Enum):\n",
    "    \"\"\"Different metrics that can be used for ranking.\"\"\"\n",
    "\n",
    "    DEGREE_TO_FREQUENCY_RATIO = 0  # Uses d(w)/f(w) as the metric\n",
    "    WORD_DEGREE = 1  # Uses d(w) alone as the metric\n",
    "    WORD_FREQUENCY = 2  # Uses f(w) alone as the metric\n",
    "\n",
    "\n",
    "class Rake(object):\n",
    "    \"\"\"Rapid Automatic Keyword Extraction Algorithm.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        stopwords=None,\n",
    "        punctuations=None,\n",
    "        language=\"english\",\n",
    "        ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO,\n",
    "        max_length=10000,\n",
    "        min_length=1,\n",
    "    ):\n",
    "        \"\"\"Constructor.\n",
    "        :param stopwords: List of Words to be ignored for keyword extraction.\n",
    "        :param punctuations: Punctuations to be ignored for keyword extraction.\n",
    "        :param language: Language to be used for stopwords\n",
    "        :param max_length: Maximum limit on the number of words in a phrase\n",
    "                           (Inclusive. Defaults to 100000)\n",
    "        :param min_length: Minimum limit on the number of words in a phrase\n",
    "                           (Inclusive. Defaults to 1)\n",
    "        \"\"\"\n",
    "        # By default use degree to frequency ratio as the metric.\n",
    "        if isinstance(ranking_metric, Metric):\n",
    "            self.metric = ranking_metric\n",
    "        else:\n",
    "            self.metric = Metric.DEGREE_TO_FREQUENCY_RATIO\n",
    "\n",
    "        # If stopwords not provided we use language stopwords by default.\n",
    "        self.stopwords = stopwords\n",
    "        if self.stopwords is None:\n",
    "            self.stopwords = nltk.corpus.stopwords.words(language)\n",
    "\n",
    "        # If punctuations are not provided we ignore all punctuation symbols.\n",
    "        self.punctuations = punctuations\n",
    "        if self.punctuations is None:\n",
    "            self.punctuations = string.punctuation\n",
    "\n",
    "        # All things which act as sentence breaks during keyword extraction.\n",
    "        self.to_ignore = set(chain(self.stopwords, self.punctuations))\n",
    "\n",
    "        # Assign min or max length to the attributes\n",
    "        self.min_length = min_length\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Stuff to be extracted from the provided text.\n",
    "        self.frequency_dist = None\n",
    "        self.degree = None\n",
    "        self.rank_list = None\n",
    "        self.ranked_phrases = None\n",
    "\n",
    "    def extract_keywords_from_text(self, text):\n",
    "        \"\"\"Method to extract keywords from the text provided.\n",
    "        :param text: Text to extract keywords from, provided as a string.\n",
    "        \"\"\"\n",
    "        sentences = nltk.tokenize.sent_tokenize(text)\n",
    "        self.extract_keywords_from_sentences(sentences)\n",
    "\n",
    "    def extract_keywords_from_sentences(self, sentences):\n",
    "        \"\"\"Method to extract keywords from the list of sentences provided.\n",
    "        :param sentences: Text to extraxt keywords from, provided as a list\n",
    "                          of strings, where each string is a sentence.\n",
    "        \"\"\"\n",
    "        phrase_list = self._generate_phrases(sentences)\n",
    "        self._build_frequency_dist(phrase_list)\n",
    "        self._build_word_co_occurance_graph(phrase_list)\n",
    "        self._build_ranklist(phrase_list)\n",
    "\n",
    "    def get_ranked_phrases(self):\n",
    "        \"\"\"Method to fetch ranked keyword strings.\n",
    "        :return: List of strings where each string represents an extracted\n",
    "                 keyword string.\n",
    "        \"\"\"\n",
    "        return self.ranked_phrases\n",
    "\n",
    "    def get_ranked_phrases_with_scores(self):\n",
    "        \"\"\"Method to fetch ranked keyword strings along with their scores.\n",
    "        :return: List of tuples where each tuple is formed of an extracted\n",
    "                 keyword string and its score. Ex: (5.68, 'Four Scoures')\n",
    "        \"\"\"\n",
    "        return self.rank_list\n",
    "\n",
    "    def get_word_frequency_distribution(self):\n",
    "        \"\"\"Method to fetch the word frequency distribution in the given text.\n",
    "        :return: Dictionary (defaultdict) of the format `word -> frequency`.\n",
    "        \"\"\"\n",
    "        return self.frequency_dist\n",
    "\n",
    "    def get_word_degrees(self):\n",
    "        \"\"\"Method to fetch the degree of words in the given text. Degree can be\n",
    "        defined as sum of co-occurances of the word with other words in the\n",
    "        given text.\n",
    "        :return: Dictionary (defaultdict) of the format `word -> degree`.\n",
    "        \"\"\"\n",
    "        return self.degree\n",
    "\n",
    "    def _build_frequency_dist(self, phrase_list):\n",
    "        \"\"\"Builds frequency distribution of the words in the given body of text.\n",
    "        :param phrase_list: List of List of strings where each sublist is a\n",
    "                            collection of words which form a contender phrase.\n",
    "        \"\"\"\n",
    "        self.frequency_dist = Counter(chain.from_iterable(phrase_list))\n",
    "\n",
    "    def _build_word_co_occurance_graph(self, phrase_list):\n",
    "        \"\"\"Builds the co-occurance graph of words in the given body of text to\n",
    "        compute degree of each word.\n",
    "        :param phrase_list: List of List of strings where each sublist is a\n",
    "                            collection of words which form a contender phrase.\n",
    "        \"\"\"\n",
    "        co_occurance_graph = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "        for phrase in phrase_list:\n",
    "            # For each phrase in the phrase list, count co-occurances of the\n",
    "            # word with other words in the phrase.\n",
    "            #\n",
    "            # Note: Keep the co-occurances graph as is, to help facilitate its\n",
    "            # use in other creative ways if required later.\n",
    "            for (word, coword) in product(phrase, phrase):\n",
    "                co_occurance_graph[word][coword] += 1\n",
    "        self.degree = defaultdict(lambda: 0)\n",
    "        for key in co_occurance_graph:\n",
    "            self.degree[key] = sum(co_occurance_graph[key].values())\n",
    "\n",
    "    def _build_ranklist(self, phrase_list):\n",
    "        \"\"\"Method to rank each contender phrase using the formula\n",
    "              phrase_score = sum of scores of words in the phrase.\n",
    "              word_score = d(w)/f(w) where d is degree and f is frequency.\n",
    "        :param phrase_list: List of List of strings where each sublist is a\n",
    "                            collection of words which form a contender phrase.\n",
    "        \"\"\"\n",
    "        self.rank_list = []\n",
    "        for phrase in phrase_list:\n",
    "            rank = 0.0\n",
    "            for word in phrase:\n",
    "                if self.metric == Metric.DEGREE_TO_FREQUENCY_RATIO:\n",
    "                    rank += 1.0 * self.degree[word] / self.frequency_dist[word]\n",
    "                elif self.metric == Metric.WORD_DEGREE:\n",
    "                    rank += 1.0 * self.degree[word]\n",
    "                else:\n",
    "                    rank += 1.0 * self.frequency_dist[word]\n",
    "            self.rank_list.append((rank, \" \".join(phrase)))\n",
    "        self.rank_list.sort(reverse=True)\n",
    "        self.ranked_phrases = [ph[1] for ph in self.rank_list]\n",
    "\n",
    "    def _generate_phrases(self, sentences):\n",
    "        \"\"\"Method to generate contender phrases given the sentences of the text\n",
    "        document.\n",
    "        :param sentences: List of strings where each string represents a\n",
    "                          sentence which forms the text.\n",
    "        :return: Set of string tuples where each tuple is a collection\n",
    "                 of words forming a contender phrase.\n",
    "        \"\"\"\n",
    "        phrase_list = set()\n",
    "        # Create contender phrases from sentences.\n",
    "        for sentence in sentences:\n",
    "            word_list = [word.lower() for word in wordpunct_tokenize(sentence)]\n",
    "            phrase_list.update(self._get_phrase_list_from_words(word_list))\n",
    "        return phrase_list\n",
    "\n",
    "    def _get_phrase_list_from_words(self, word_list):\n",
    "        \"\"\"Method to create contender phrases from the list of words that form\n",
    "        a sentence by dropping stopwords and punctuations and grouping the left\n",
    "        words into phrases. Only phrases in the given length range (both limits\n",
    "        inclusive) would be considered to build co-occurrence matrix. Ex:\n",
    "        Sentence: Red apples, are good in flavour.\n",
    "        List of words: ['red', 'apples', \",\", 'are', 'good', 'in', 'flavour']\n",
    "        List after dropping punctuations and stopwords.\n",
    "        List of words: ['red', 'apples', *, *, good, *, 'flavour']\n",
    "        List of phrases: [('red', 'apples'), ('good',), ('flavour',)]\n",
    "        List of phrases with a correct length:\n",
    "        For the range [1, 2]: [('red', 'apples'), ('good',), ('flavour',)]\n",
    "        For the range [1, 1]: [('good',), ('flavour',)]\n",
    "        For the range [2, 2]: [('red', 'apples')]\n",
    "        :param word_list: List of words which form a sentence when joined in\n",
    "                          the same order.\n",
    "        :return: List of contender phrases that are formed after dropping\n",
    "                 stopwords and punctuations.\n",
    "        \"\"\"\n",
    "        groups = groupby(word_list, lambda x: x not in self.to_ignore)\n",
    "        phrases = [tuple(group[1]) for group in groups if group[0]]\n",
    "        return list(\n",
    "            filter(\n",
    "                lambda x: self.min_length <= len(x) <= self.max_length, phrases\n",
    "            )\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Helper functions\n",
    "###########################\n",
    "\n",
    "# Generate frequency dictionary of all words in all reviews (needed for TF-IDF)\n",
    "def generateCorpus(X):\n",
    "    all_words = defaultdict(int)\n",
    "    for k in range(0,len(X)):\n",
    "\t\twordList = re.sub(\"[^\\w]\", \" \",  X[k]).split() # Clean and split data\n",
    "\t\tfor words in wordList:\n",
    "\t\t\tall_words[words.lower()] += 1\n",
    "    \n",
    "    return all_words\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t, 'n') for t in word_tokenize(doc)]\n",
    "\n",
    "def TF_IDF_all(asin_dict):\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words=\"english\", analyzer='word', lowercase = True, tokenizer = LemmaTokenizer(), \n",
    "\t\t\t\t ngram_range=(1, 2), min_df = 2, max_df = 0.8)\n",
    "\n",
    "    vec = vectorizer.fit_transform(list(asin_dict.values()))\n",
    "    word_map=vectorizer.get_feature_names()\n",
    "\n",
    "    return vec, word_map\n",
    "\n",
    "def TF_IDF_asin(asin, asin_dict, vectorizer, word_map, n):\n",
    "    index = asin_dict.keys().index(asin)\n",
    "    tf = vectorizer[index]\n",
    "    \n",
    "    keywords = []\n",
    "    for col in tf.nonzero()[1]:\n",
    "        keywords.append((tf[0, col],word_map[col]))\n",
    "\n",
    "    sorted_keywords = sorted(keywords, key=lambda t: t[1] * -1)\n",
    "\n",
    "    return sorted_keywords[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'unicode'>\n",
      "<type 'dict'>\n",
      "<type 'unicode'>\n",
      "<type 'dict'>\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in asin_mother_dict.keys():\n",
    "    if count< 2:\n",
    "#         print asin_mother_dict[i]['pos_reviews']\n",
    "        print type (asin_mother_dict[i])\n",
    "#         print type (asin_mother_dict[i])\n",
    "        count+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin_dict= {}\n",
    "for i in asin_mother_dict.keys():\n",
    "#     print type(asin_mother_dict[i])\n",
    "#     print (asin_mother_dict[i]['pos_reviews'])\n",
    "    if asin_mother_dict[i]['all_param_known']=='True' :\n",
    "        asin_dict[i]= asin_mother_dict[i]['pos_reviews']\n",
    "\n",
    "number_of_asins_on_which_keyword_separators_should_be_applied= len(asin_dict.keys())\n",
    "counter=0\n",
    "rake_keywords_dict={}\n",
    "TFIDF_keywords_dict={}\n",
    "vec, map = TF_IDF_all(asin_dict)\n",
    "\n",
    "for i in asin_dict.keys():\n",
    "    if counter < number_of_asins_on_which_keyword_separators_should_be_applied:\n",
    "#         print asin_dict[i]\n",
    "        c = Rake()\n",
    "        c.extract_keywords_from_text(asin_dict[i])\n",
    "        rake_keywords_dict[i]= c.rank_list\n",
    "        counter+=1\n",
    "        \n",
    "        ###########################\n",
    "        # Run TF-IDF\n",
    "        ###########################\n",
    "#         #test_list = [0, 2, 3, 4, 8, 9, 12, 13, 14]\n",
    "#         test_list = [0]\n",
    "\n",
    "#         for i in test_list:\n",
    "#             my_asin = asin_dict.keys()[i]\n",
    "#             print my_asin\n",
    "#         print asin_title_map[i]\n",
    "\n",
    "        TFIDF_keywords_dict[i] = TF_IDF_asin(i, asin_dict, vec, map, 10)\n",
    "#             print keywords\n",
    "    else:\n",
    "        break\n",
    "        #     rl_list.append(c.rank_list)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "# d = Rake()\n",
    "# d.extract_keywords_from_text('I love cats and cats')\n",
    "# print d.rank_list\n",
    "# print d.ranked_phrases\n",
    "print len(TFIDF_keywords_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:33: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:36: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:75: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n",
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:89: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:78: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srishti/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:47: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    }
   ],
   "source": [
    "#converts reviews to embedding dict where key is the ASIN of a book\n",
    "count=0\n",
    "RAKE_review_dict={}\n",
    "TFIDF_review_dict={}\n",
    "\n",
    "lower_cutoff_for_word_to_be_considered=1#model parameter\n",
    "TFIDF_lower_cutoff_for_word_to_be_considered=0.0#model parameter\n",
    "\n",
    "number_of_reviews_for_clustering=100#select how many reviews are being used for clustering, small number reduces computation time. \n",
    "\n",
    "max_word_per_review= 10#model parameter\n",
    "TFIDF_max_word_per_review= 10#model parameter\n",
    "\n",
    "\n",
    "if number_of_reviews_for_clustering > len(rake_keywords_dict.keys()):\n",
    "    print (\"number_of_reviews_for_clustering too large!!\")\n",
    "    print(\"number of reviews= \" + str(len(rake_keywords_dict.keys()))+ \" requested \" + str(number_of_reviews_for_clustering))\n",
    "else:\n",
    "    for i in rake_keywords_dict.keys():\n",
    "        normalization_factor=0\n",
    "#         print i\n",
    "        if count< number_of_reviews_for_clustering :\n",
    "            print count\n",
    "            count +=1\n",
    "    #         print rake_keywords_dict[i]\n",
    "            count_of_words_used=0\n",
    "#             print \"entering RAKE\"\n",
    "            for j in rake_keywords_dict[i]:\n",
    "#                 print \"entered RAKE\"\n",
    "                if count_of_words_used < max_word_per_review:\n",
    "                    count_of_words_used+=1\n",
    "                    if j[0] > lower_cutoff_for_word_to_be_considered:\n",
    "                        if j[1] not in embeddings_index.keys():\n",
    "                            for k in j[1].split():\n",
    "                #                 print (\"k=\" + k)\n",
    "                                if k in embeddings_index.keys():\n",
    "                                    if i not in RAKE_review_dict.keys():\n",
    "\n",
    "                                        RAKE_review_dict[i]= j[0]* embeddings_index[k] \n",
    "                                        normalization_factor+=1\n",
    "                                    else:\n",
    "                                        RAKE_review_dict[i]+= j[0]* embeddings_index[k] \n",
    "                                        normalization_factor+=1\n",
    "        #                         else:\n",
    "        #                             print k\n",
    "                        else:\n",
    "                            if j[1] in embeddings_index.keys():\n",
    "                #                 print (\"j=\" + j[1])\n",
    "                    #             RAKE_review_dict[i]+= rl_list[j][0]* embeddings_index[j][1]\n",
    "                                if i not in RAKE_review_dict.keys():\n",
    "                                    RAKE_review_dict[i]= j[0]* embeddings_index[j[1]]\n",
    "                                    normalization_factor+=1\n",
    "                                else:\n",
    "                                    RAKE_review_dict[i]+= j[0]* embeddings_index[j[1]]\n",
    "                                    normalization_factor+=1\n",
    "        #                     else:\n",
    "        #                         print j[1]\n",
    "#             print (\"entering normalization\" + str(i))\n",
    "            if normalization_factor >0:\n",
    "#                     RAKE_review_dict[i]= RAKE_review_dict[i]/ normalization_factor\n",
    "                [z/normalization_factor for z in RAKE_review_dict[i]]\n",
    "#             print (\"RAKE done..\")\n",
    "\n",
    "    #TF-IDF converts reviews to embedding dict where key is the ASIN of a book\n",
    "#             print (\"RAKE done..\")\n",
    "            TFIDF_normalization_factor=0\n",
    "            TFIDF_count_of_words_used=0\n",
    "#             print TFIDF_keywords_dict[i]\n",
    "#             print \"entering TFIDF\"\n",
    "            for j in TFIDF_keywords_dict[i]:\n",
    "                \n",
    "                if TFIDF_count_of_words_used < max_word_per_review:\n",
    "                    TFIDF_count_of_words_used+=1\n",
    "                    if j[0] > TFIDF_lower_cutoff_for_word_to_be_considered:\n",
    "                        if j[1] not in embeddings_index.keys():\n",
    "                            for k in j[1].split():\n",
    "#                                 print (\"k=\" + k)\n",
    "                                if k in embeddings_index.keys():\n",
    "#                                     print (\"k=\" + k)\n",
    "                                    if i not in TFIDF_review_dict.keys():\n",
    "                                        TFIDF_review_dict[i]= j[0]* embeddings_index[k] \n",
    "                                        TFIDF_normalization_factor+=1\n",
    "                                    else:\n",
    "                                        TFIDF_review_dict[i]+= j[0]* embeddings_index[k] \n",
    "                                        TFIDF_normalization_factor+=1\n",
    "        #                         else:\n",
    "        #                             print k\n",
    "                        else:\n",
    "                            if j[1] in embeddings_index.keys():\n",
    "#                                 print (\"j=\" + j[1])\n",
    "                                if i not in TFIDF_review_dict.keys():\n",
    "                                    TFIDF_review_dict[i]= j[0]* embeddings_index[j[1]]\n",
    "                                    TFIDF_normalization_factor+=1\n",
    "                                else:\n",
    "                                    TFIDF_review_dict[i]+= j[0]* embeddings_index[j[1]]\n",
    "                                    TFIDF_normalization_factor+=1\n",
    "        #                     else:\n",
    "        #                         print j[1]\n",
    "#             print (\"entering TFIDF Normalization\" + str(i))\n",
    "#             print TFIDF_review_dict\n",
    "            if TFIDF_normalization_factor >0:\n",
    "#                     RAKE_review_dict[i]= RAKE_review_dict[i]/ normalization_factor\n",
    "                [z/TFIDF_normalization_factor for z in TFIDF_review_dict[i]]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print len(TFIDF_review_dict.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "count =0\n",
    "for i in RAKE_review_dict.keys():\n",
    "    if i not in TFIDF_review_dict.keys():\n",
    "        print i\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAKE_review_dict_list=[]\n",
    "TFIDF_review_dict_list=[]\n",
    "for i in RAKE_review_dict.keys():\n",
    "    RAKE_review_dict_list.append(RAKE_review_dict[i])\n",
    "    TFIDF_review_dict_list.append(TFIDF_review_dict[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 2 7 1 1 7 1 3 4 4 4 1 7 4 1 7 1 1 1 4 1 5 3 3 7 3 1 7 3 3 3 7 0 7 7 1 3\n",
      " 7 1 1 3 7 3 7 3 9 3 1 6 7 3 1 5 7 6 7 3 1 1 1 7 7 5 4 7 1 7 7 3 1 1 1 1 9\n",
      " 8 1 7 6 7 1 7 4 7 1 4 4 7 4 7 4 7 1 1 1 1 7 7 4 7 1]\n",
      "[0 5 6 1 5 7 5 3 3 2 2 9 0 0 0 3 9 3 0 3 9 3 0 5 2 0 2 5 0 3 0 3 0 3 4 3 3\n",
      " 3 5 3 3 9 9 0 0 3 0 5 0 0 5 5 0 3 0 0 0 6 5 3 3 3 0 0 3 1 6 0 3 3 8 3 3 0\n",
      " 0 0 3 0 3 3 3 0 3 1 3 3 3 3 0 0 3 9 9 3 5 3 3 0 4 3]\n"
     ]
    }
   ],
   "source": [
    "#holy K-means\n",
    "from sklearn.cluster import KMeans\n",
    "RAKE_kmeans = KMeans(n_clusters=10, random_state=0).fit_predict(RAKE_review_dict_list)\n",
    "TFIDF_kmeans= KMeans(n_clusters=10, random_state=0).fit_predict(TFIDF_review_dict_list)\n",
    "print RAKE_kmeans\n",
    "print TFIDF_kmeans\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAKE_Clustered Titles\n",
      "TFIDF_Clustered Titles\n"
     ]
    }
   ],
   "source": [
    "#matches the embeddings of review to a title\n",
    "RAKE_clusteredTitles= collections.defaultdict(list)\n",
    "RAKE_clustered_asins= collections.defaultdict(list)\n",
    "for i in range(0, len(RAKE_kmeans)):\n",
    "    for asin_value, word2VecList in RAKE_review_dict.items():\n",
    "        if (word2VecList == RAKE_review_dict_list[i]).all():\n",
    "#             print asin_value\n",
    "            RAKE_clusteredTitles[RAKE_kmeans[i]].append(asin_mother_dict[asin_value]['title'])\n",
    "#             temp_dict={}\n",
    "#             temp_dict['asins']= RAKE_clustered_asins[RAKE_kmeans[i]].append(asin_value)\n",
    "            RAKE_clustered_asins[RAKE_kmeans[i]].append(asin_value)\n",
    "            break\n",
    "print (\"RAKE_Clustered Titles\")\n",
    "# print clusteredTitles\n",
    "\n",
    "TFIDF_clusteredTitles= collections.defaultdict(list)\n",
    "TFIDF_clustered_asins= collections.defaultdict(list)\n",
    "for i in range(0, len(TFIDF_kmeans)):\n",
    "    for asin_value, word2VecList in TFIDF_review_dict.items():\n",
    "        if (word2VecList == TFIDF_review_dict_list[i]).all():\n",
    "#             print asin_value\n",
    "            TFIDF_clusteredTitles[TFIDF_kmeans[i]].append(asin_mother_dict[asin_value]['title'])\n",
    "            TFIDF_clustered_asins[TFIDF_kmeans[i]].append(asin_value)\n",
    "            break\n",
    "print (\"TFIDF_Clustered Titles\")\n",
    "# print TFIDF_clusteredTitles\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAKE_Cluster Number: 0[u'B00008PW1R']\n",
      "RAKE_Cluster Number: 1[u'B000052Y44', u'B0000537YG', u'B00009LI88', u'B0000C4KJ9', u'B000052YFO', u'B00008WFLP', u'B00008IJYS', u'B0000C4KJX', u'B000050B6Z', u'B000050B6B', u'B0000530YM', u'B00005NFBC', u'B00007MII0', u'B000052XKL', u'B00008O2XM', u'B000052YCG', u'B000050FE2', u'B00009ZOHD', u'B0000532OS', u'B000092YOX', u'B0000E3J9G', u'B0000E3J9F', u'B0000E3J9B', u'B00005335B', u'B000099SKD', u'B0000AN9LO', u'B0000AQOEW', u'B00002N88C', u'B000052YIY', u'B0000AU1ZU', u'B000087BHJ']\n",
      "RAKE_Cluster Number: 2[u'3812028492']\n",
      "RAKE_Cluster Number: 3[u'B000052Y5U', u'B00007GD5L', u'B0000DAZ2G', u'B000052YOH', u'B000052YOB', u'B0000DGBI2', u'B00008K4UA', u'B000053072', u'B000065CIL', u'B00007MII1', u'B00005BAWO', u'B00006IV1N', u'B0000DAPGS', u'B00005317T', u'B00005V3D9', u'B000052X6U']\n",
      "RAKE_Cluster Number: 4[u'B000052YF7', u'B000052YHR', u'B000052YHS', u'B000052YFJ', u'B0000531E2', u'B00007KUX7', u'B00008LUTV', u'B000092YP2', u'B000092YP1', u'B000053136', u'B00006IV4N', u'B0000U1OCI']\n",
      "RAKE_Cluster Number: 5[u'B0000CFXSL', u'B0000532VC', u'B0000532OE']\n",
      "RAKE_Cluster Number: 6[u'B0000AZWI8', u'B0000AN9L7', u'B00003IEME']\n",
      "RAKE_Cluster Number: 7[u'B0000E2E9S', u'B0000SW9GK', u'B0000DH8OC', u'B0000533I2', u'B000050B6D', u'B00009R7VT', u'B000066PGS', u'B00004T7UJ', u'B00004Z4AE', u'B00006BSXM', u'B00006IV1V', u'B000067NMJ', u'B00006K116', u'B0000CBIWC', u'B000052YCL', u'B00005UVD9', u'B0000E3J99', u'B00007KUX6', u'B00005IBVP', u'B0000CFH36', u'B0000CFNVN', u'B000099SKB', u'B000099SKE', u'B0000E5YZI', u'B0000DG5BK', u'B0000534X3', u'B00005TP4H', u'B000059S7H', u'B00009PSZ2', u'B00004Z4EG']\n",
      "RAKE_Cluster Number: 8[u'B000052X6F']\n",
      "RAKE_Cluster Number: 9[u'B0000536Y7', u'B000052X6G']\n",
      "TFIDF_Cluster Number: 0[u'B000052Y5U', u'B0000DH8OC', u'B000052YFJ', u'B000052YFO', u'B0000C4KJX', u'B0000DAZ2G', u'B000052YOB', u'B0000DGBI2', u'B000053072', u'B00008PW1R', u'B000067NMJ', u'B00006IV1N', u'B0000DAPGS', u'B0000AZWI8', u'B00006K116', u'B0000532VC', u'B0000AN9L7', u'B000052YCL', u'B00005V3D9', u'B0000532OE', u'B00007KUX7', u'B0000CFH36', u'B000052X6G', u'B000052X6F', u'B00005335B', u'B00003IEME', u'B00008LUTV', u'B0000534X3', u'B00006IV4N', u'B0000U1OCI']\n",
      "TFIDF_Cluster Number: 1[u'B000052Y44', u'B0000532OS', u'B0000AN9LO']\n",
      "TFIDF_Cluster Number: 2[u'B000052YHR', u'B000052YHS', u'B000050B6D', u'B000050B6B']\n",
      "TFIDF_Cluster Number: 3[u'B00007GD5L', u'B000052YF7', u'B0000533I2', u'B00008IJYS', u'B0000531E2', u'B0000CFXSL', u'B00008K4UA', u'B000066PGS', u'B00004T7UJ', u'B0000530YM', u'B000065CIL', u'B00006BSXM', u'B00007MII0', u'B00007MII1', u'B0000536Y7', u'B0000CBIWC', u'B00009ZOHD', u'B00005UVD9', u'B0000E3J99', u'B00007KUX6', u'B000052X6U', u'B000092YOX', u'B0000E3J9F', u'B0000E3J9B', u'B0000CFNVN', u'B000099SKB', u'B000099SKD', u'B000099SKE', u'B0000E5YZI', u'B000092YP2', u'B000092YP1', u'B0000DG5BK', u'B000053136', u'B00005TP4H', u'B000052YIY', u'B000059S7H', u'B00009PSZ2', u'B000087BHJ']\n",
      "TFIDF_Cluster Number: 4[u'B00004Z4AE', u'B00004Z4EG']\n",
      "TFIDF_Cluster Number: 5[u'3812028492', u'B0000537YG', u'B00009LI88', u'B000052YOH', u'B00009R7VT', u'B00005NFBC', u'B000052XKL', u'B00005317T', u'B00008O2XM', u'B000050FE2', u'B0000AU1ZU']\n",
      "TFIDF_Cluster Number: 6[u'B0000E2E9S', u'B000052YCG', u'B00005IBVP']\n",
      "TFIDF_Cluster Number: 7[u'B0000SW9GK']\n",
      "TFIDF_Cluster Number: 8[u'B0000E3J9G']\n",
      "TFIDF_Cluster Number: 9[u'B0000C4KJ9', u'B00008WFLP', u'B000050B6Z', u'B00006IV1V', u'B00005BAWO', u'B0000AQOEW', u'B00002N88C']\n"
     ]
    }
   ],
   "source": [
    "for i in RAKE_clusteredTitles.keys():\n",
    "    print \"RAKE_Cluster Number: \" + str(i)+ str(RAKE_clustered_asins[i])\n",
    "\n",
    "for i in TFIDF_clusteredTitles.keys():\n",
    "    print \"TFIDF_Cluster Number: \" + str(i)+ str(TFIDF_clustered_asins[i])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createOutputDicts(clustered_asins_dict):\n",
    "#Create output dicts\n",
    "    output_dict={}\n",
    "    for i in clustered_asins_dict.keys():\n",
    "        temp_dict={}\n",
    "        temp_dict['asins']= clustered_asins_dict[i]\n",
    "        output_dict[i]= temp_dict\n",
    "    return output_dict\n",
    "    # print RAKE_output_dict\n",
    "\n",
    "# TFIDF_output_dict={}\n",
    "# for i in TFIDF_clustered_asins.keys():\n",
    "#     temp_dict={}\n",
    "#     temp_dict['asins']= TFIDF_clustered_asins[i]\n",
    "#     TFIDF_output_dict[i]= temp_dict\n",
    "# print TFIDF_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAKE_output_dict= createOutputDicts(RAKE_clustered_asins)\n",
    "TFIDF_output_dict=createOutputDicts(TFIDF_clustered_asins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'asins': [u'B00008PW1R']}, 1: {'asins': [u'B000052Y44', u'B0000537YG', u'B00009LI88', u'B0000C4KJ9', u'B000052YFO', u'B00008WFLP', u'B00008IJYS', u'B0000C4KJX', u'B000050B6Z', u'B000050B6B', u'B0000530YM', u'B00005NFBC', u'B00007MII0', u'B000052XKL', u'B00008O2XM', u'B000052YCG', u'B000050FE2', u'B00009ZOHD', u'B0000532OS', u'B000092YOX', u'B0000E3J9G', u'B0000E3J9F', u'B0000E3J9B', u'B00005335B', u'B000099SKD', u'B0000AN9LO', u'B0000AQOEW', u'B00002N88C', u'B000052YIY', u'B0000AU1ZU', u'B000087BHJ']}, 2: {'asins': [u'3812028492']}, 3: {'asins': [u'B000052Y5U', u'B00007GD5L', u'B0000DAZ2G', u'B000052YOH', u'B000052YOB', u'B0000DGBI2', u'B00008K4UA', u'B000053072', u'B000065CIL', u'B00007MII1', u'B00005BAWO', u'B00006IV1N', u'B0000DAPGS', u'B00005317T', u'B00005V3D9', u'B000052X6U']}, 4: {'asins': [u'B000052YF7', u'B000052YHR', u'B000052YHS', u'B000052YFJ', u'B0000531E2', u'B00007KUX7', u'B00008LUTV', u'B000092YP2', u'B000092YP1', u'B000053136', u'B00006IV4N', u'B0000U1OCI']}, 5: {'asins': [u'B0000CFXSL', u'B0000532VC', u'B0000532OE']}, 6: {'asins': [u'B0000AZWI8', u'B0000AN9L7', u'B00003IEME']}, 7: {'asins': [u'B0000E2E9S', u'B0000SW9GK', u'B0000DH8OC', u'B0000533I2', u'B000050B6D', u'B00009R7VT', u'B000066PGS', u'B00004T7UJ', u'B00004Z4AE', u'B00006BSXM', u'B00006IV1V', u'B000067NMJ', u'B00006K116', u'B0000CBIWC', u'B000052YCL', u'B00005UVD9', u'B0000E3J99', u'B00007KUX6', u'B00005IBVP', u'B0000CFH36', u'B0000CFNVN', u'B000099SKB', u'B000099SKE', u'B0000E5YZI', u'B0000DG5BK', u'B0000534X3', u'B00005TP4H', u'B000059S7H', u'B00009PSZ2', u'B00004Z4EG']}, 8: {'asins': [u'B000052X6F']}, 9: {'asins': [u'B0000536Y7', u'B000052X6G']}}\n",
      "{0: {'asins': [u'B000052Y5U', u'B0000DH8OC', u'B000052YFJ', u'B000052YFO', u'B0000C4KJX', u'B0000DAZ2G', u'B000052YOB', u'B0000DGBI2', u'B000053072', u'B00008PW1R', u'B000067NMJ', u'B00006IV1N', u'B0000DAPGS', u'B0000AZWI8', u'B00006K116', u'B0000532VC', u'B0000AN9L7', u'B000052YCL', u'B00005V3D9', u'B0000532OE', u'B00007KUX7', u'B0000CFH36', u'B000052X6G', u'B000052X6F', u'B00005335B', u'B00003IEME', u'B00008LUTV', u'B0000534X3', u'B00006IV4N', u'B0000U1OCI']}, 1: {'asins': [u'B000052Y44', u'B0000532OS', u'B0000AN9LO']}, 2: {'asins': [u'B000052YHR', u'B000052YHS', u'B000050B6D', u'B000050B6B']}, 3: {'asins': [u'B00007GD5L', u'B000052YF7', u'B0000533I2', u'B00008IJYS', u'B0000531E2', u'B0000CFXSL', u'B00008K4UA', u'B000066PGS', u'B00004T7UJ', u'B0000530YM', u'B000065CIL', u'B00006BSXM', u'B00007MII0', u'B00007MII1', u'B0000536Y7', u'B0000CBIWC', u'B00009ZOHD', u'B00005UVD9', u'B0000E3J99', u'B00007KUX6', u'B000052X6U', u'B000092YOX', u'B0000E3J9F', u'B0000E3J9B', u'B0000CFNVN', u'B000099SKB', u'B000099SKD', u'B000099SKE', u'B0000E5YZI', u'B000092YP2', u'B000092YP1', u'B0000DG5BK', u'B000053136', u'B00005TP4H', u'B000052YIY', u'B000059S7H', u'B00009PSZ2', u'B000087BHJ']}, 4: {'asins': [u'B00004Z4AE', u'B00004Z4EG']}, 5: {'asins': [u'3812028492', u'B0000537YG', u'B00009LI88', u'B000052YOH', u'B00009R7VT', u'B00005NFBC', u'B000052XKL', u'B00005317T', u'B00008O2XM', u'B000050FE2', u'B0000AU1ZU']}, 6: {'asins': [u'B0000E2E9S', u'B000052YCG', u'B00005IBVP']}, 7: {'asins': [u'B0000SW9GK']}, 8: {'asins': [u'B0000E3J9G']}, 9: {'asins': [u'B0000C4KJ9', u'B00008WFLP', u'B000050B6Z', u'B00006IV1V', u'B00005BAWO', u'B0000AQOEW', u'B00002N88C']}}\n"
     ]
    }
   ],
   "source": [
    "print RAKE_output_dict\n",
    "print TFIDF_output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAKE_output_dict=updateOutputDict(RAKE_output_dict,asin_mother_dict)\n",
    "TFIDF_output_dict= updateOutputDict(TFIDF_output_dict, asin_mother_dict)\n",
    "# print RAKE_output_dict\n",
    "# print TFIDF_output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findTopCategories(category_dict, number_of_categories_to_pick):\n",
    "    list_top_counts=[]\n",
    "    list_top_categories=[]\n",
    "    dict_top={}\n",
    "    number_of_dict_picked= number_of_categories_to_pick\n",
    "    for i in category_dict.keys():\n",
    "        if len(list_top_counts) < number_of_dict_picked:\n",
    "            list_top_counts.append(category_dict[i])\n",
    "            list_top_categories.append(i)\n",
    "        else:\n",
    "            min_value= min(list_top_counts)\n",
    "            if  min_value < category_dict[i]:\n",
    "                \n",
    "#                 print i +\" \" + str(category_dict[i])\n",
    "#                 print list_top_four_categories\n",
    "                list_top_counts[list_top_counts.index(min_value)]= category_dict[i]\n",
    "                for j in list_top_categories:\n",
    "                    if category_dict[j]== min_value:\n",
    "                        list_top_categories[list_top_categories.index(j)]= i\n",
    "                        break\n",
    "                        \n",
    "#     print(\"printing selected categories in helper function: \" + str(list_top_four_categories))                \n",
    "    for i in list_top_categories:\n",
    "        dict_top[i]= category_dict[i]\n",
    "#     print(\"printing selected categories in helper function: \" + str(dict_top_four))\n",
    "    return dict_top\n",
    "    \n",
    "#                 print \"category_dict\"+ str(category_dict)\n",
    "#                 print list_top_four_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputSummary(output_dict, mother_dict):\n",
    "    output_summary_dict={}\n",
    "    for i in output_dict:\n",
    "        count_also_viewed=0\n",
    "        count_also_bought=0\n",
    "        count_bought_together=0\n",
    "        count_buy_after_viewing=0\n",
    "        asin_count_in_cluster=0\n",
    "\n",
    "        temp_dict={}\n",
    "        for j in output_dict[i]['asins']:\n",
    "            asin_count_in_cluster+=1\n",
    "            if j in output_dict[i]['also_viewed']:\n",
    "                count_also_viewed+=1\n",
    "            if j in output_dict[i]['also_bought']:\n",
    "                count_also_bought+=1\n",
    "            if j in output_dict[i]['bought_together']:\n",
    "                count_bought_together+=1\n",
    "            if j in output_dict[i]['buy_after_viewing']:\n",
    "                count_buy_after_viewing+=1\n",
    "\n",
    "            dict_category_distribution={}\n",
    "            for k in mother_dict[j]['categories']:\n",
    "    #             print k\n",
    "    #             print dict_category_distribution.keys()\n",
    "                if k not in dict_category_distribution.keys():\n",
    "                    dict_category_distribution[k]=1\n",
    "                else:\n",
    "                    dict_category_distribution[k]+=1   \n",
    "    #     print \"dict_category_distribution for cluster \" + str(i)+ \":\"+ str(dict_category_distribution)\n",
    "        top_categories=findTopCategories(dict_category_distribution,3)\n",
    "    #     print \"top_four_categories for cluster \"+ str(i)+ \":\"+ str(top_four_categories)\n",
    "\n",
    "#         temp_dict[\"also_viewed\"]= count_also_viewed\n",
    "#         temp_dict[\"also_bought\"]= count_also_bought\n",
    "#         temp_dict[\"bought_together\"]= count_bought_together\n",
    "#         temp_dict[\"buy_after_viewing\"]= count_buy_after_viewing\n",
    "        temp_dict[\"asin_count_in_cluster\"]= asin_count_in_cluster\n",
    "        temp_dict[\"top_categories\"]= top_categories\n",
    "        temp_dict[\"count_related_products\"]= count_also_bought+count_also_viewed+count_bought_together+count_buy_after_viewing\n",
    "\n",
    "        output_summary_dict[i]=temp_dict\n",
    "    return output_summary_dict\n",
    "\n",
    "    #     print (\"cluster:\" + str(i)+ \" \"+ \"also_viewed\"+ str(count_also_viewed)+\" \"+ \"also_bought\"+\n",
    "    #            str(count_also_bought)+\" \"+\"bought_together\"+str(count_bought_together)+\" \"+\n",
    "    #            \"buy_after_viewing\"+ str(count_buy_after_viewing)+ \" \"+ \"asin count in cluster\"+ str(asin_count_in_cluster))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAKE_output_summary_dict= outputSummary(RAKE_output_dict, asin_mother_dict)\n",
    "TFIDF_output_summary_dict= outputSummary(TFIDF_output_dict, asin_mother_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'asin_count_in_cluster': 1, 'count_related_products': 0, 'top_categories': {'Health & Personal Care': 1, 'Health Care': 1, 'Non-Aspirin': 1}}, 1: {'asin_count_in_cluster': 31, 'count_related_products': 4, 'top_categories': {'Health & Personal Care': 1, 'Health Care': 1, \"Women's Health\": 1}}, 2: {'asin_count_in_cluster': 1, 'count_related_products': 0, 'top_categories': {'Shaving & Hair Removal': 1, 'Health & Personal Care': 1, 'Manual Shaving': 1}}, 3: {'asin_count_in_cluster': 16, 'count_related_products': 0, 'top_categories': {'Health & Personal Care': 1, 'Health Care': 1, 'Sinus Medicine': 1}}, 4: {'asin_count_in_cluster': 12, 'count_related_products': 6, 'top_categories': {'Health & Personal Care': 1, 'Vitamins & Dietary Supplements': 1, 'Vitamin B': 1}}, 5: {'asin_count_in_cluster': 3, 'count_related_products': 0, 'top_categories': {'Health & Personal Care': 1, 'Health Care': 1, 'Absorbent Pads, Shields, Guards & Liners': 1}}, 6: {'asin_count_in_cluster': 3, 'count_related_products': 0, 'top_categories': {'AA': 1, 'Health & Personal Care': 1, 'Household Supplies': 1}}, 7: {'asin_count_in_cluster': 30, 'count_related_products': 6, 'top_categories': {'Health & Personal Care': 1, 'Scouring Pads': 1, 'Dishwashing': 1}}, 8: {'asin_count_in_cluster': 1, 'count_related_products': 0, 'top_categories': {'Health & Personal Care': 1, 'Health Care': 1, 'Sinus Medicine': 1}}, 9: {'asin_count_in_cluster': 2, 'count_related_products': 0, 'top_categories': {'Health & Personal Care': 1, 'Health Care': 1, 'Moisturizing Sprays': 1}}}\n",
      "{0: {'asin_count_in_cluster': 30, 'count_related_products': 7, 'top_categories': {'Health & Personal Care': 1, 'Vitamins & Dietary Supplements': 1, 'Vitamin B': 1}}, 1: {'asin_count_in_cluster': 3, 'count_related_products': 0, 'top_categories': {'Health & Personal Care': 1, 'Bath Tissue': 1, 'Household Supplies': 1}}, 2: {'asin_count_in_cluster': 4, 'count_related_products': 5, 'top_categories': {'Shaving & Hair Removal': 1, 'Health & Personal Care': 1, 'Rotary Shavers': 1}}, 3: {'asin_count_in_cluster': 38, 'count_related_products': 20, 'top_categories': {'Health & Personal Care': 1, 'Health Care': 1, \"Women's Health\": 1}}, 4: {'asin_count_in_cluster': 2, 'count_related_products': 3, 'top_categories': {'Health & Personal Care': 1, 'Scouring Pads': 1, 'Dishwashing': 1}}, 5: {'asin_count_in_cluster': 11, 'count_related_products': 0, 'top_categories': {'Medical Supplies & Equipment': 1, 'Health & Personal Care': 1, 'Elbow Braces': 1}}, 6: {'asin_count_in_cluster': 3, 'count_related_products': 0, 'top_categories': {'Health & Personal Care': 1, 'Protein': 1, 'Powders': 1}}, 7: {'asin_count_in_cluster': 1, 'count_related_products': 0, 'top_categories': {'Health & Personal Care': 1, 'Health Care': 1, 'Hemorrhoid Care': 1}}, 8: {'asin_count_in_cluster': 1, 'count_related_products': 0, 'top_categories': {'Health & Personal Care': 1, 'Health Care': 1, 'Heating Pads': 1}}, 9: {'asin_count_in_cluster': 7, 'count_related_products': 0, 'top_categories': {'Health & Personal Care': 1, 'Cleaning Tools': 1, 'Buckets': 1}}}\n"
     ]
    }
   ],
   "source": [
    "print RAKE_output_summary_dict\n",
    "print TFIDF_output_summary_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
